<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.4.0 -->
<title>Generating pseudo random text with Markov chains using Python | Agiliq Blogs</title>
<meta name="generator" content="Jekyll v3.7.3" />
<meta property="og:title" content="Generating pseudo random text with Markov chains using Python" />
<meta name="author" content="shabda" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="First the definition from Wolfram A Markov chain is collection of random variables {X_t} (where the index t runs through 0, 1, ...) having the property that, given the present, the future is conditionally independent of the past. Wikipedia is a little clearer ...Markov chain is a stochastic process with markov property ... [Which means] state changes are probabilistic, and future state depend on current state only. Markov chains have various uses, but now let’s see how it can be used to generate gibberish, which might look legit. The algorithm is, Have a text which will serve as the corpus from which we choose the next transitions. Start with two consecutive words from the text. The last two words constitute the present state. Generating next word is the markov transition. To generate the next word, look in the corpus, and find which words are present after the given two words. Choose one of them randomly. Repeat 2, until text of required size is generated. The code for this is To see a sample output, we take the text of My man jeeves by Wodehouse from Project Gutenberg, and see a sample output. In [1]: file_ = open(&#39;/home/shabda/jeeves.txt&#39;) In [2]: import markovgen In [3]: markov = markovgen.Markov(file_) In [4]: markov.generate_markov_text() Out[4]: &#39;Can you put a few years of your twin-brother Alfred, who was apt to rally round a bit. I should strongly advocate the blue with milk&#39; [The files you need to run this are jeeves.txt and markovgen.py] How is this a markov algorithm? The last two words are the current state. Next word depends on last two words only, or on present state only. The next word is randomly chosen from a statistical model of the corpus. Here is some sample text. “The quick brown fox jumps over the brown fox who is slow jumps over the brown fox who is dead.” This gives us a corpus like, {(&#39;The&#39;, &#39;quick&#39;): [&#39;brown&#39;], (&#39;brown&#39;, &#39;fox&#39;): [&#39;jumps&#39;, &#39;who&#39;, &#39;who&#39;], (&#39;fox&#39;, &#39;jumps&#39;): [&#39;over&#39;], (&#39;fox&#39;, &#39;who&#39;): [&#39;is&#39;, &#39;is&#39;], (&#39;is&#39;, &#39;slow&#39;): [&#39;jumps&#39;], (&#39;jumps&#39;, &#39;over&#39;): [&#39;the&#39;, &#39;the&#39;], (&#39;over&#39;, &#39;the&#39;): [&#39;brown&#39;, &#39;brown&#39;], (&#39;quick&#39;, &#39;brown&#39;): [&#39;fox&#39;], (&#39;slow&#39;, &#39;jumps&#39;): [&#39;over&#39;], (&#39;the&#39;, &#39;brown&#39;): [&#39;fox&#39;, &#39;fox&#39;], (&#39;who&#39;, &#39;is&#39;): [&#39;slow&#39;, &#39;dead.&#39;]} Now if we start with “brown fox”, the next word can be “jumps” or “who”. If we choose “jumps”, then the current state is “fox jumps” and next word is over, and so on. Hints The larger text we choose, the more choices are there at each transition, and a better looking text is generated. The state can be set to depend on one word, two words, or any number of words. As the number of words in each state increases, the generated text becomes less random. Don’t strip the punctuations etc. They lead to a more representative corpus, and a better random text. Resources jeeves.txt markovgen.py Online markov text generator Twitter markov script Did you know that markov chains have many other uses in finance, statistics and mathematics? In fact Google’s page rank algorithm is essentially a markov chain of the web! We publish articles on Algorithms, Python, Django and related technologies every week. Why not Subscribe to us or follow us on twitter?" />
<meta property="og:description" content="First the definition from Wolfram A Markov chain is collection of random variables {X_t} (where the index t runs through 0, 1, ...) having the property that, given the present, the future is conditionally independent of the past. Wikipedia is a little clearer ...Markov chain is a stochastic process with markov property ... [Which means] state changes are probabilistic, and future state depend on current state only. Markov chains have various uses, but now let’s see how it can be used to generate gibberish, which might look legit. The algorithm is, Have a text which will serve as the corpus from which we choose the next transitions. Start with two consecutive words from the text. The last two words constitute the present state. Generating next word is the markov transition. To generate the next word, look in the corpus, and find which words are present after the given two words. Choose one of them randomly. Repeat 2, until text of required size is generated. The code for this is To see a sample output, we take the text of My man jeeves by Wodehouse from Project Gutenberg, and see a sample output. In [1]: file_ = open(&#39;/home/shabda/jeeves.txt&#39;) In [2]: import markovgen In [3]: markov = markovgen.Markov(file_) In [4]: markov.generate_markov_text() Out[4]: &#39;Can you put a few years of your twin-brother Alfred, who was apt to rally round a bit. I should strongly advocate the blue with milk&#39; [The files you need to run this are jeeves.txt and markovgen.py] How is this a markov algorithm? The last two words are the current state. Next word depends on last two words only, or on present state only. The next word is randomly chosen from a statistical model of the corpus. Here is some sample text. “The quick brown fox jumps over the brown fox who is slow jumps over the brown fox who is dead.” This gives us a corpus like, {(&#39;The&#39;, &#39;quick&#39;): [&#39;brown&#39;], (&#39;brown&#39;, &#39;fox&#39;): [&#39;jumps&#39;, &#39;who&#39;, &#39;who&#39;], (&#39;fox&#39;, &#39;jumps&#39;): [&#39;over&#39;], (&#39;fox&#39;, &#39;who&#39;): [&#39;is&#39;, &#39;is&#39;], (&#39;is&#39;, &#39;slow&#39;): [&#39;jumps&#39;], (&#39;jumps&#39;, &#39;over&#39;): [&#39;the&#39;, &#39;the&#39;], (&#39;over&#39;, &#39;the&#39;): [&#39;brown&#39;, &#39;brown&#39;], (&#39;quick&#39;, &#39;brown&#39;): [&#39;fox&#39;], (&#39;slow&#39;, &#39;jumps&#39;): [&#39;over&#39;], (&#39;the&#39;, &#39;brown&#39;): [&#39;fox&#39;, &#39;fox&#39;], (&#39;who&#39;, &#39;is&#39;): [&#39;slow&#39;, &#39;dead.&#39;]} Now if we start with “brown fox”, the next word can be “jumps” or “who”. If we choose “jumps”, then the current state is “fox jumps” and next word is over, and so on. Hints The larger text we choose, the more choices are there at each transition, and a better looking text is generated. The state can be set to depend on one word, two words, or any number of words. As the number of words in each state increases, the generated text becomes less random. Don’t strip the punctuations etc. They lead to a more representative corpus, and a better random text. Resources jeeves.txt markovgen.py Online markov text generator Twitter markov script Did you know that markov chains have many other uses in finance, statistics and mathematics? In fact Google’s page rank algorithm is essentially a markov chain of the web! We publish articles on Algorithms, Python, Django and related technologies every week. Why not Subscribe to us or follow us on twitter?" />
<link rel="canonical" href="http://localhost:4000/algorithms/2009/06/16/generating-pseudo-random-text-with-markov-chains-u.html" />
<meta property="og:url" content="http://localhost:4000/algorithms/2009/06/16/generating-pseudo-random-text-with-markov-chains-u.html" />
<meta property="og:site_name" content="Agiliq Blogs" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2009-06-16T11:54:11+05:30" />
<script type="application/ld+json">
{"description":"First the definition from Wolfram A Markov chain is collection of random variables {X_t} (where the index t runs through 0, 1, ...) having the property that, given the present, the future is conditionally independent of the past. Wikipedia is a little clearer ...Markov chain is a stochastic process with markov property ... [Which means] state changes are probabilistic, and future state depend on current state only. Markov chains have various uses, but now let’s see how it can be used to generate gibberish, which might look legit. The algorithm is, Have a text which will serve as the corpus from which we choose the next transitions. Start with two consecutive words from the text. The last two words constitute the present state. Generating next word is the markov transition. To generate the next word, look in the corpus, and find which words are present after the given two words. Choose one of them randomly. Repeat 2, until text of required size is generated. The code for this is To see a sample output, we take the text of My man jeeves by Wodehouse from Project Gutenberg, and see a sample output. In [1]: file_ = open(&#39;/home/shabda/jeeves.txt&#39;) In [2]: import markovgen In [3]: markov = markovgen.Markov(file_) In [4]: markov.generate_markov_text() Out[4]: &#39;Can you put a few years of your twin-brother Alfred, who was apt to rally round a bit. I should strongly advocate the blue with milk&#39; [The files you need to run this are jeeves.txt and markovgen.py] How is this a markov algorithm? The last two words are the current state. Next word depends on last two words only, or on present state only. The next word is randomly chosen from a statistical model of the corpus. Here is some sample text. “The quick brown fox jumps over the brown fox who is slow jumps over the brown fox who is dead.” This gives us a corpus like, {(&#39;The&#39;, &#39;quick&#39;): [&#39;brown&#39;], (&#39;brown&#39;, &#39;fox&#39;): [&#39;jumps&#39;, &#39;who&#39;, &#39;who&#39;], (&#39;fox&#39;, &#39;jumps&#39;): [&#39;over&#39;], (&#39;fox&#39;, &#39;who&#39;): [&#39;is&#39;, &#39;is&#39;], (&#39;is&#39;, &#39;slow&#39;): [&#39;jumps&#39;], (&#39;jumps&#39;, &#39;over&#39;): [&#39;the&#39;, &#39;the&#39;], (&#39;over&#39;, &#39;the&#39;): [&#39;brown&#39;, &#39;brown&#39;], (&#39;quick&#39;, &#39;brown&#39;): [&#39;fox&#39;], (&#39;slow&#39;, &#39;jumps&#39;): [&#39;over&#39;], (&#39;the&#39;, &#39;brown&#39;): [&#39;fox&#39;, &#39;fox&#39;], (&#39;who&#39;, &#39;is&#39;): [&#39;slow&#39;, &#39;dead.&#39;]} Now if we start with “brown fox”, the next word can be “jumps” or “who”. If we choose “jumps”, then the current state is “fox jumps” and next word is over, and so on. Hints The larger text we choose, the more choices are there at each transition, and a better looking text is generated. The state can be set to depend on one word, two words, or any number of words. As the number of words in each state increases, the generated text becomes less random. Don’t strip the punctuations etc. They lead to a more representative corpus, and a better random text. Resources jeeves.txt markovgen.py Online markov text generator Twitter markov script Did you know that markov chains have many other uses in finance, statistics and mathematics? In fact Google’s page rank algorithm is essentially a markov chain of the web! We publish articles on Algorithms, Python, Django and related technologies every week. Why not Subscribe to us or follow us on twitter?","author":{"@type":"Person","name":"shabda"},"@type":"BlogPosting","url":"http://localhost:4000/algorithms/2009/06/16/generating-pseudo-random-text-with-markov-chains-u.html","headline":"Generating pseudo random text with Markov chains using Python","dateModified":"2009-06-16T11:54:11+05:30","datePublished":"2009-06-16T11:54:11+05:30","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/algorithms/2009/06/16/generating-pseudo-random-text-with-markov-chains-u.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Agiliq Blogs" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Agiliq Blogs</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a><a class="page-link" href="/categories/forms.html">Forms</a><a class="page-link" href="/categories/interviews.html">Interviews</a><a class="page-link" href="/categories/marketing.html">Marketing</a><a class="page-link" href="/categories/paypal.html">Paypal</a><a class="page-link" href="/categories/python.html">Python</a><a class="page-link" href="/categories/search.html">Search</a><a class="page-link" href="/categories/startup.html">Startup</a><a class="page-link" href="/categories/uncategorized.html">Uncategorized</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Generating pseudo random text with Markov chains using Python</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2009-06-16T11:54:11+05:30" itemprop="datePublished">Jun 16, 2009
      </time>• <span itemprop="author" itemscope itemtype="http://schema.org/Person"><span class="p-author h-card" itemprop="name">shabda</span></span></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>First the definition from <a href="http://mathworld.wolfram.com/MarkovChain.html">Wolfram</a></p>

<blockquote>
A Markov chain is collection of random variables {X_t} (where the index t runs through 0, 1, ...) having the property that, given the present, the future is conditionally independent of the past.
</blockquote>

<p><a href="http://en.wikipedia.org/wiki/Transition_probabilities">Wikipedia</a> is a little clearer</p>

<blockquote>
 ...Markov chain is a stochastic process with markov property ... [Which means] state changes are probabilistic, and future state depend on current state only.
</blockquote>

<p>Markov chains have various uses, but now let’s see how it can be used to generate
gibberish, which might look legit.</p>

<p>The algorithm is,</p>

<ol>
  <li>Have a text which will serve as the corpus from which we choose the next
transitions.</li>
  <li>Start with two consecutive words from the text. The last two words constitute
the present state.</li>
  <li>Generating next word is the markov transition. To generate the next word, look
in the corpus, and find which words are present after the given two words. Choose
one of them randomly.</li>
  <li>Repeat 2, until text of required size is generated.</li>
</ol>

<p>The code for this is</p>

<script src="http://gist.github.com/131679.js"></script>

<p>To see a sample output, we take the text of <em>My man jeeves</em> by Wodehouse from
<a href="http://www.gutenberg.org/etext/8164">Project Gutenberg</a>, and see a sample output.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>In [1]: file_ = open('/home/shabda/jeeves.txt')

In [2]: import markovgen

In [3]: markov = markovgen.Markov(file_)

In [4]: markov.generate_markov_text()
Out[4]: 'Can you put a few years of your twin-brother Alfred,
who was apt to rally round a bit. I should strongly advocate
the blue with milk'
</code></pre></div></div>

<p>[The files you need to run this are <a href="http://uswaretech.com/blog/wp-content/uploads/2009/06/jeeves.txt">jeeves.txt</a> and <a href="http://uswaretech.com/blog/wp-content/uploads/2009/06/markovgenpy.txt">markovgen.py</a>]</p>

<h3 id="how-is-this-a-markov-algorithm">How is this a markov algorithm?</h3>

<ul>
  <li>The last two words are the current state.</li>
  <li>Next word depends on last two words only, or on <em>present state</em> only.</li>
  <li>The next word is <em>randomly chosen</em> from a statistical model of the corpus.</li>
</ul>

<p>Here is some sample text.</p>

<p>“The quick brown fox jumps over the brown fox who is slow jumps over the brown
fox who is dead.”</p>

<p>This gives us a corpus like,</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{('The', 'quick'): ['brown'],
 ('brown', 'fox'): ['jumps', 'who', 'who'],
 ('fox', 'jumps'): ['over'],
 ('fox', 'who'): ['is', 'is'],
 ('is', 'slow'): ['jumps'],
 ('jumps', 'over'): ['the', 'the'],
 ('over', 'the'): ['brown', 'brown'],
 ('quick', 'brown'): ['fox'],
 ('slow', 'jumps'): ['over'],
 ('the', 'brown'): ['fox', 'fox'],
 ('who', 'is'): ['slow', 'dead.']}
</code></pre></div></div>

<p>Now if we start with “brown fox”, the next word can be “jumps” or “who”. If we
 choose “jumps”, then the current state is “fox jumps” and next word is over,
 and so on.</p>

<h3 id="hints">Hints</h3>

<ul>
  <li>The larger text we choose, the more choices are there at each transition, and
a better looking text is generated.</li>
  <li>The state can be set to depend on one word, two words, or any number of words.
As the number of words in each state increases, the generated text becomes less
random.</li>
  <li>Don’t strip the punctuations etc. They lead to a more representative corpus,
and a better random text.</li>
</ul>

<h3 id="resources">Resources</h3>

<ul>
  <li><a href="http://uswaretech.com/blog/wp-content/uploads/2009/06/jeeves.txt">jeeves.txt</a></li>
  <li><a href="http://uswaretech.com/blog/wp-content/uploads/2009/06/markovgenpy.txt">markovgen.py</a></li>
  <li><a href="http://www.yisongyue.com/shaney/">Online markov text generator</a></li>
  <li><a href="http://www.yaymukund.com/twittov/">Twitter markov script</a></li>
</ul>

<hr />

<p>Did you know that markov chains have many other uses in finance, statistics and mathematics? In fact Google’s page rank algorithm is essentially a markov chain of the web! We publish articles on Algorithms, Python, Django and related technologies every week. Why not <a href="http://agiliq.com/newsletter/subscribe/">Subscribe to us</a> or <a href="http://twitter.com/agiliqdotcom">follow us on twitter</a>?</p>


  </div><a class="u-url" href="/algorithms/2009/06/16/generating-pseudo-random-text-with-markov-chains-u.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Agiliq Blogs</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Agiliq Blogs</li><li><a class="u-email" href="mailto:your-email@example.com">your-email@example.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/jekyll"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">jekyll</span></a></li><li><a href="https://www.twitter.com/jekyllrb"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">jekyllrb</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
