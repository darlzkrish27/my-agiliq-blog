<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">

<!-- Begin Jekyll SEO tag v2.4.0 -->
<title>How performant is your Python web application | Agiliq Blogs</title>
<meta name="generator" content="Jekyll v3.7.3" />
<meta property="og:title" content="How performant is your Python web application" />
<meta name="author" content="akshar" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This post tries to explain web application performance. Performance means the number of requests per second that can be served by a deployed application. This post would help answer questions like: How performant is an application. How much load can it handle. How many concurrent requests can it serve. How can you determine requests per second for an application. What steps to take to increase serving capability for an application. This post has as much code as theory. This post assumes that you have a basic understanding of processes, threads. You can read our previous post for basic understanding of processes and threads. In any web application there are many urls and associated handlers/controllers for each url. A url might respond in 200ms while another url might take 3 seconds. While determining performance of an application, choose the url which will be used most often and determine its performance. Factors which determine performance There are many factors determining performance of an application. Major factors are: Application type Application complexity Web server Physical server, i.e infrastructure Web server configuration In this post we will see how changing web server configuration changes performance with other factors remaining constant. Two major components of web server configuration are: Number of processes/workers Number of threads We will also see what web server configuration should be preferred for which application type. Example: How increasing number of server processes boosts performance for some applications while it reduces performance for other application types. Application type Applications could be compute intensive or network intensive or I/O intensive. Compute intensive applications can’t get any benefit by making number of server workers or threads greater than number of CPU cores. Network intensive or I/O intensive applications can benefit a lot by making several server workers/threads run on each core. Application complexity Suppose a url handler makes two db calls and takes a second to respond. In such case reducing number of db calls to 1 will reduce response time by half and number of requests served per second will get doubled. Web server There are many web servers. eg: Apache, Gunicorn, uwsgi etc. Apache might be better than gunicorn and might be able to handle more requests per second than gunicorn. Physical server/Infrastructure Increasing the number of cores or memory will improve the performance. If a single core machine is able to handle 10 requests per second for a computationally intensive application, then a machine with 2 cores should be able to handle 20 requests per second. This might need properly configuring the web server to get maximum utilization from physical server. Application server configuration Number of running web server processes influences performance. Similarly number of server threads in each process influences performance too. Demo server configuration Our demo physical server is a t1.micro instance with 1 GB RAM. It has a single core. Demo application for this post uses Django/Python served from a Gunicorn application server. Familiarity with Django would be helpful but you should be able to follow as long as you have understanding of any web framework. Demo application Our application has the following url: http://34.233.117.92:8000/static_content_sleep The Django handler/controller for url static_content_sleep looks like: def static_content_sleep(request): st = time.time() # Compute Start time print &quot;sleeping&quot; time.sleep(1) # Simulate db call which takes 1 seconds print &quot;waking&quot; print time.time() - st, &quot;in this function&quot; # Compute end time return HttpResponse(&quot;Woke up&quot;) Most web applications would be working with a database to fetch data. We don’t have a database setup. We are assuming that db used by the handler responds in 1 second. We are simulating a db call by making the code execution sleep for 1 second. This application is being served by Gunicorn web server. Gunicorn configuration looks like: workers = 1 threads = 1 bind = &#39;0.0.0.0:8000&#39; daemon = False The important configuration variables are workers and threads. Ignore others. If you want to setup gunicorn on your physical server, you can refer our previous post. We used default gunicorn configuration for workers and threads. The default configuration of gunicorn has following characteristics: workers Default is 1. This means that only one gunicorn process would be running on the physical server. If we make it 2, it would mean that 2 gunicorn processes would be running on the server. threads Default is 1. This tells number of threads in each worker process. This means that each gunicorn worker is single threaded and isn’t multithreaded. Making requests Let’s make two simultaneous request to this url. Making two requests from browser will not be simultaneous as switching from one browser tab to another tab might take you more than a second. In that case you cannot properly observe time taken by the server to process two simultaneous requests. Let’s write a Python function to make simultaneous requests and log the time. In [15]: from threading import Thread In [16]: import requests In [17]: class UrlThread(Thread): ...: def run(self): ...: resp = requests.get(&#39;http://34.233.117.92:8000/static_content_sleep&#39;) In [18]: def make_n_requests(num_requests): ...: threads = [] ...: for i in range(num_requests): ...: threads.append(UrlThread()) ...: start = time.time() # The requests will be almost simultaneous. # Second request will be made within nanoseconds of making the first request. ...: for thread in threads: ...: thread.start() # Threads will be started without waiting for response of previous threads ...: for thread in threads: ...: thread.join() # Wait for response for all the requests. ...: end = time.time() ...: print &quot;Time to get response for %d simultaneous requests&quot; % (num_requests,), end - start You can use curl or a shell script or any programming language using which you can simulate n simultaneous requests. Make 2 simultaneous requests In [19]: make_n_requests(2) Time to get response for 2 simultaneous requests 2.622205019 Gunicorn log on server looks like: sleeping waking 1.00131487846 in this function sleeping waking 1.00130581856 in this function Observation Print statements of the handler are printed. Print statements are printed again. Based on prints, we can infer that second request’s execution started after completion of first request. Let’s make 5 simultaneous requests and see what happens In [20]: make_n_requests(5) Time to get response for 5 simultaneous requests 5.65813708305 Gunicorn log looks like: sleeping waking 1.00114989281 in this function sleeping waking 1.0011370182 in this function sleeping waking 1.00114202499 in this function sleeping waking 1.00130295753 in this function sleeping waking 1.00131988525 in this function You can see 5 sets of print statements. Observation As we increase number of simultaneous requests, response time is proportionately increasing. For 2 requests, server takes around 2 seconds to respond. For 5 requests, server takes around 5 seconds to respond. For 10 requests, it would take around 10 seconds to respond. Let’s change gunicorn threadsfrom its default value of 1 to 5. threads = 5 Gunicorn process will have 5 threads now. And each thread is capable of serving a request. Hence 5 requests can be simultaneosly served. Restart gunicorn. Let’s make 5 simultaneous requests again. In [27]: make_n_requests(5) Time to get response for 5 simultaneous requests 1.77244091034 ** While earlier it was taking around 5 seconds to get 5 simultaneous requests processed, now it takes less than 2 seconds. ** Server log looks like: sleeping sleeping sleeping sleeping sleeping waking 1.001278162 in this function waking 1.00262784958 in this function waking 1.00353288651 in this function waking 1.00325298309 in this function waking 1.0035841465 in this function Important Observation Each gunicorn thread could handle a request. Processor kept switching between all these 5 threads. This way all 5 requests being processed by 5 different threads got a chance to execute concurrently without waiting for completion of any request. Print pattern also suggests that execution of 5 requests started before waiting for completion of first request. We can accomplish this performance boost by increasing number of workers instead of number of threads. We will change threads back to 1 and increase workers to 5 instead. workers = 5 threads = 1 Make 5 simultaneous requests In [22]: make_n_requests(5) Time to get response for 5 simultaneous requests 1.76864600182 Server gunicorn log looks like: sleeping sleeping sleeping sleeping sleeping waking 1.00134015083 in this function waking 1.00125384331 in this function waking 1.00277590752 in this function waking 1.00121593475 in this function waking 1.00092220306 in this function Here, processor executed 5 processes parallely. Let’s make 10 simultaneous requests: In [23]: make_n_requests(10) Time to get response for 10 simultaneous requests 2.74328804016 Server log looks like: sleeping -&gt; First 5 requests are assigned to 5 workers sleeping sleeping sleeping sleeping waking -&gt; One worker, say worker 1, finished execution. waking -&gt; Another worker, say worker 2, finished execution. This print was executed before last print statement of worker 1 could execute. 1.00396203995 in this function -&gt; Worker 1 last print statement. Worker 1 free now and can serve another request. 1.00669813156 in this function -&gt; Worker 2 last print statement. Worker 2 free now sleeping -&gt; Assigned to probably worker 1 waking -&gt; Probably worker 3 woke up. 1.00510120392 in this function sleeping waking 1.00605106354 in this function sleeping sleeping waking 1.00563693047 in this function sleeping waking 1.00345993042 in this function waking 1.00495409966 in this function waking 1.00393104553 in this function waking 1.0027141571 in this function waking 1.00151586533 in this function Observation We made 10 simultaneous requests. First 5 requests were assigned to 5 running gunicorn processes. First 5 requests were concurrently handled during 1st second. 5 remaining requests were waiting to be executed during this time. During next second each gunicorn process picked up another request after completing a request. That’s why it took around 2 seconds to get response for 10 requests. Let’s use 2 threads with 5 workers threads = 2 workers = 5 Let’s restart gunicorn. Now there are 5 gunicorn processes running and each process is running 2 threads. In [16]: make_n_requests(10) Time to get response for 10 simultaneous requests 1.46275486946 Server log looks like: sleeping -&gt; &#39;sleeping&#39; repeated 10 times before any &#39;waking&#39;. sleeping sleeping sleeping sleeping sleeping sleeping sleeping sleeping sleeping waking 1.00119280815 in this function waking 1.00499987602 in this function waking 1.00233006477 in this function waking 1.00144195557 in this function waking 1.00114202499 in this function waking 1.00117397308 in this function waking 1.00114512444 in this function waking 1.00120997429 in this function waking 1.00124192238 in this function waking 1.00117397308 in this function Inference Based on these examples we can infer that making number of threads/workers greater than number of cores improves the performance for a network intensive and I/O intensive application. Can we keep increasing workers and threads You have to keep RAM usage under consideration while tuning the number of workers and threads. Code execution for a handler needs memory. While worker is processing a request, sufficient memory must be available. If handling each request needs 50 MB and you have 5 workers and 1 thread running, you must ensure that 250 MB free RAM is there after starting gunicorn. Performance with current configuration Curent configuration has 2 workers and 5 threads for each worker. So 10 requests will be handled concurrently. Each request takes around 1 second to respond. So with current configuration server can handle 10 requests per second. If the handler is changed to time.sleep(0.5), i.e if each request could respond in approximately 0.5 seconds then server would be able to handle 20 requests per second. If we change number of workers to 3 and 5 threads for each worker, then with time.sleep(0.5), server would be able to handle 3*5*2, i,e 30 requests per second. Increase performance by increasing number of cores Suppose we use a machine with 2 cores. On single core machine if we were using 2 workers, then on double core machine we should use 4 workers. This assumes that there is sufficient RAM available to be allocated to 4 workers. A dual core machine would be able to handle 4*5, i.e 20 requests per second, assuming each request responds in a second. If we found that 3 workers with 5 thread each is an optimum combination on a single core machine, then we should use 6 workers on a dual core machine. Computationally bound applications Earlier sections discussed about I/O bound applications. Let’s talk about CPU bound applications. App has a following url: http://34.233.117.92:8000/list_of_dict The Django handler/controller for url list_of_dict looks like: def list_of_dict(request): print &quot;entered function&quot; st = time.time() for i in xrange(30000000): pass print time.time() - st, &quot;in this function&quot; return HttpResponse(json.dumps(&quot;a&quot;)) Let’s change UrlThread used by make_n_requests to work with this new url. In [14]: class UrlThread(Thread): ...: def run(self): ...: resp = requests.get(&#39;http://34.233.117.92:8000/list_of_dict&#39;) Let’s change gunicorn configuration to have a single worker and single thread. workers = 1 threads = 1 Let’s make 1 request to this url In [28]: make_n_requests(1) Time to get response for 1 simultaneous requests 1.49780297279 Server log looks like: entered function 0.454895019531 in this function It takes around 0.5 seconds in the handler as you can see from server log. And it takes around 1 second for request and response to travel on the wire. So in total it takes around 1.5 seconds. Let’s make 5 requests to this url In [32]: make_n_requests(5) Time to get response for 5 simultaneous requests 2.94885587692 Server log looks like: entered function 0.460360050201 in this function entered function 0.459032058716 in this function entered function 0.462526082993 in this function entered function 0.45965385437 in this function entered function 0.460576057434 in this function Each request takes around 0.5 seconds to complete. Plus there is an additional time for request and response to move over the wire. In total it takes around 3 seconds. Time to get response for n requests is increasing linearly as we increase n. With a single worker and single thread, time to get response for simultaneous requests was increasing linearly with number of requests in I/O bound handler too. Let’s change gunicorn threadsfrom its default value of 1 to 5. threads = 5 Let’s make 5 simultaneous requests again. In [33]: make_n_requests(5) Time to get response for 5 simultaneous requests 3.37198781967 We didn’t get any advantage by increasing number of threads. Instead the performance deteriorated. Server log looks like: entered function entered function entered function entered function entered function 2.68351387978 in this function 2.67835998535 in this function 2.67423701286 in this function 2.67234015465 in this function 2.6794462204 in this function Event different requests didn’t complete in 0.5 second as was happening with single thread. The CPU time was split between 5 threads and so instead of 0.5 seconds it took 2.6 seconds for each request to complete. Observation In CPU bound applications, CPU isn’t idle. So multiple threads don’t provide an advantage. CPU time is split between threads, which infact leads to longer response time for each request. Thread switching brings a performance hit. That’s why we saw response time for 5 requests going up from 2.9 seconds to 3.3 seconds for 5 simultaneous requests. Takeaways There is no definite answer for how many workers and threads will provide maximum performance. It’s a matter of tuning and finding out. Making number of workers/threads greater than number of cores almost always leads to higher number of requests per second handling in a I/O bound application. Making number of workers/threads greater than number of cores almost always leads to reduced number of requests per second handling in a CPU bound application. Making number of workers/threads greater than number of cores leads to increase in response time for each request in a CPU bound application." />
<meta property="og:description" content="This post tries to explain web application performance. Performance means the number of requests per second that can be served by a deployed application. This post would help answer questions like: How performant is an application. How much load can it handle. How many concurrent requests can it serve. How can you determine requests per second for an application. What steps to take to increase serving capability for an application. This post has as much code as theory. This post assumes that you have a basic understanding of processes, threads. You can read our previous post for basic understanding of processes and threads. In any web application there are many urls and associated handlers/controllers for each url. A url might respond in 200ms while another url might take 3 seconds. While determining performance of an application, choose the url which will be used most often and determine its performance. Factors which determine performance There are many factors determining performance of an application. Major factors are: Application type Application complexity Web server Physical server, i.e infrastructure Web server configuration In this post we will see how changing web server configuration changes performance with other factors remaining constant. Two major components of web server configuration are: Number of processes/workers Number of threads We will also see what web server configuration should be preferred for which application type. Example: How increasing number of server processes boosts performance for some applications while it reduces performance for other application types. Application type Applications could be compute intensive or network intensive or I/O intensive. Compute intensive applications can’t get any benefit by making number of server workers or threads greater than number of CPU cores. Network intensive or I/O intensive applications can benefit a lot by making several server workers/threads run on each core. Application complexity Suppose a url handler makes two db calls and takes a second to respond. In such case reducing number of db calls to 1 will reduce response time by half and number of requests served per second will get doubled. Web server There are many web servers. eg: Apache, Gunicorn, uwsgi etc. Apache might be better than gunicorn and might be able to handle more requests per second than gunicorn. Physical server/Infrastructure Increasing the number of cores or memory will improve the performance. If a single core machine is able to handle 10 requests per second for a computationally intensive application, then a machine with 2 cores should be able to handle 20 requests per second. This might need properly configuring the web server to get maximum utilization from physical server. Application server configuration Number of running web server processes influences performance. Similarly number of server threads in each process influences performance too. Demo server configuration Our demo physical server is a t1.micro instance with 1 GB RAM. It has a single core. Demo application for this post uses Django/Python served from a Gunicorn application server. Familiarity with Django would be helpful but you should be able to follow as long as you have understanding of any web framework. Demo application Our application has the following url: http://34.233.117.92:8000/static_content_sleep The Django handler/controller for url static_content_sleep looks like: def static_content_sleep(request): st = time.time() # Compute Start time print &quot;sleeping&quot; time.sleep(1) # Simulate db call which takes 1 seconds print &quot;waking&quot; print time.time() - st, &quot;in this function&quot; # Compute end time return HttpResponse(&quot;Woke up&quot;) Most web applications would be working with a database to fetch data. We don’t have a database setup. We are assuming that db used by the handler responds in 1 second. We are simulating a db call by making the code execution sleep for 1 second. This application is being served by Gunicorn web server. Gunicorn configuration looks like: workers = 1 threads = 1 bind = &#39;0.0.0.0:8000&#39; daemon = False The important configuration variables are workers and threads. Ignore others. If you want to setup gunicorn on your physical server, you can refer our previous post. We used default gunicorn configuration for workers and threads. The default configuration of gunicorn has following characteristics: workers Default is 1. This means that only one gunicorn process would be running on the physical server. If we make it 2, it would mean that 2 gunicorn processes would be running on the server. threads Default is 1. This tells number of threads in each worker process. This means that each gunicorn worker is single threaded and isn’t multithreaded. Making requests Let’s make two simultaneous request to this url. Making two requests from browser will not be simultaneous as switching from one browser tab to another tab might take you more than a second. In that case you cannot properly observe time taken by the server to process two simultaneous requests. Let’s write a Python function to make simultaneous requests and log the time. In [15]: from threading import Thread In [16]: import requests In [17]: class UrlThread(Thread): ...: def run(self): ...: resp = requests.get(&#39;http://34.233.117.92:8000/static_content_sleep&#39;) In [18]: def make_n_requests(num_requests): ...: threads = [] ...: for i in range(num_requests): ...: threads.append(UrlThread()) ...: start = time.time() # The requests will be almost simultaneous. # Second request will be made within nanoseconds of making the first request. ...: for thread in threads: ...: thread.start() # Threads will be started without waiting for response of previous threads ...: for thread in threads: ...: thread.join() # Wait for response for all the requests. ...: end = time.time() ...: print &quot;Time to get response for %d simultaneous requests&quot; % (num_requests,), end - start You can use curl or a shell script or any programming language using which you can simulate n simultaneous requests. Make 2 simultaneous requests In [19]: make_n_requests(2) Time to get response for 2 simultaneous requests 2.622205019 Gunicorn log on server looks like: sleeping waking 1.00131487846 in this function sleeping waking 1.00130581856 in this function Observation Print statements of the handler are printed. Print statements are printed again. Based on prints, we can infer that second request’s execution started after completion of first request. Let’s make 5 simultaneous requests and see what happens In [20]: make_n_requests(5) Time to get response for 5 simultaneous requests 5.65813708305 Gunicorn log looks like: sleeping waking 1.00114989281 in this function sleeping waking 1.0011370182 in this function sleeping waking 1.00114202499 in this function sleeping waking 1.00130295753 in this function sleeping waking 1.00131988525 in this function You can see 5 sets of print statements. Observation As we increase number of simultaneous requests, response time is proportionately increasing. For 2 requests, server takes around 2 seconds to respond. For 5 requests, server takes around 5 seconds to respond. For 10 requests, it would take around 10 seconds to respond. Let’s change gunicorn threadsfrom its default value of 1 to 5. threads = 5 Gunicorn process will have 5 threads now. And each thread is capable of serving a request. Hence 5 requests can be simultaneosly served. Restart gunicorn. Let’s make 5 simultaneous requests again. In [27]: make_n_requests(5) Time to get response for 5 simultaneous requests 1.77244091034 ** While earlier it was taking around 5 seconds to get 5 simultaneous requests processed, now it takes less than 2 seconds. ** Server log looks like: sleeping sleeping sleeping sleeping sleeping waking 1.001278162 in this function waking 1.00262784958 in this function waking 1.00353288651 in this function waking 1.00325298309 in this function waking 1.0035841465 in this function Important Observation Each gunicorn thread could handle a request. Processor kept switching between all these 5 threads. This way all 5 requests being processed by 5 different threads got a chance to execute concurrently without waiting for completion of any request. Print pattern also suggests that execution of 5 requests started before waiting for completion of first request. We can accomplish this performance boost by increasing number of workers instead of number of threads. We will change threads back to 1 and increase workers to 5 instead. workers = 5 threads = 1 Make 5 simultaneous requests In [22]: make_n_requests(5) Time to get response for 5 simultaneous requests 1.76864600182 Server gunicorn log looks like: sleeping sleeping sleeping sleeping sleeping waking 1.00134015083 in this function waking 1.00125384331 in this function waking 1.00277590752 in this function waking 1.00121593475 in this function waking 1.00092220306 in this function Here, processor executed 5 processes parallely. Let’s make 10 simultaneous requests: In [23]: make_n_requests(10) Time to get response for 10 simultaneous requests 2.74328804016 Server log looks like: sleeping -&gt; First 5 requests are assigned to 5 workers sleeping sleeping sleeping sleeping waking -&gt; One worker, say worker 1, finished execution. waking -&gt; Another worker, say worker 2, finished execution. This print was executed before last print statement of worker 1 could execute. 1.00396203995 in this function -&gt; Worker 1 last print statement. Worker 1 free now and can serve another request. 1.00669813156 in this function -&gt; Worker 2 last print statement. Worker 2 free now sleeping -&gt; Assigned to probably worker 1 waking -&gt; Probably worker 3 woke up. 1.00510120392 in this function sleeping waking 1.00605106354 in this function sleeping sleeping waking 1.00563693047 in this function sleeping waking 1.00345993042 in this function waking 1.00495409966 in this function waking 1.00393104553 in this function waking 1.0027141571 in this function waking 1.00151586533 in this function Observation We made 10 simultaneous requests. First 5 requests were assigned to 5 running gunicorn processes. First 5 requests were concurrently handled during 1st second. 5 remaining requests were waiting to be executed during this time. During next second each gunicorn process picked up another request after completing a request. That’s why it took around 2 seconds to get response for 10 requests. Let’s use 2 threads with 5 workers threads = 2 workers = 5 Let’s restart gunicorn. Now there are 5 gunicorn processes running and each process is running 2 threads. In [16]: make_n_requests(10) Time to get response for 10 simultaneous requests 1.46275486946 Server log looks like: sleeping -&gt; &#39;sleeping&#39; repeated 10 times before any &#39;waking&#39;. sleeping sleeping sleeping sleeping sleeping sleeping sleeping sleeping sleeping waking 1.00119280815 in this function waking 1.00499987602 in this function waking 1.00233006477 in this function waking 1.00144195557 in this function waking 1.00114202499 in this function waking 1.00117397308 in this function waking 1.00114512444 in this function waking 1.00120997429 in this function waking 1.00124192238 in this function waking 1.00117397308 in this function Inference Based on these examples we can infer that making number of threads/workers greater than number of cores improves the performance for a network intensive and I/O intensive application. Can we keep increasing workers and threads You have to keep RAM usage under consideration while tuning the number of workers and threads. Code execution for a handler needs memory. While worker is processing a request, sufficient memory must be available. If handling each request needs 50 MB and you have 5 workers and 1 thread running, you must ensure that 250 MB free RAM is there after starting gunicorn. Performance with current configuration Curent configuration has 2 workers and 5 threads for each worker. So 10 requests will be handled concurrently. Each request takes around 1 second to respond. So with current configuration server can handle 10 requests per second. If the handler is changed to time.sleep(0.5), i.e if each request could respond in approximately 0.5 seconds then server would be able to handle 20 requests per second. If we change number of workers to 3 and 5 threads for each worker, then with time.sleep(0.5), server would be able to handle 3*5*2, i,e 30 requests per second. Increase performance by increasing number of cores Suppose we use a machine with 2 cores. On single core machine if we were using 2 workers, then on double core machine we should use 4 workers. This assumes that there is sufficient RAM available to be allocated to 4 workers. A dual core machine would be able to handle 4*5, i.e 20 requests per second, assuming each request responds in a second. If we found that 3 workers with 5 thread each is an optimum combination on a single core machine, then we should use 6 workers on a dual core machine. Computationally bound applications Earlier sections discussed about I/O bound applications. Let’s talk about CPU bound applications. App has a following url: http://34.233.117.92:8000/list_of_dict The Django handler/controller for url list_of_dict looks like: def list_of_dict(request): print &quot;entered function&quot; st = time.time() for i in xrange(30000000): pass print time.time() - st, &quot;in this function&quot; return HttpResponse(json.dumps(&quot;a&quot;)) Let’s change UrlThread used by make_n_requests to work with this new url. In [14]: class UrlThread(Thread): ...: def run(self): ...: resp = requests.get(&#39;http://34.233.117.92:8000/list_of_dict&#39;) Let’s change gunicorn configuration to have a single worker and single thread. workers = 1 threads = 1 Let’s make 1 request to this url In [28]: make_n_requests(1) Time to get response for 1 simultaneous requests 1.49780297279 Server log looks like: entered function 0.454895019531 in this function It takes around 0.5 seconds in the handler as you can see from server log. And it takes around 1 second for request and response to travel on the wire. So in total it takes around 1.5 seconds. Let’s make 5 requests to this url In [32]: make_n_requests(5) Time to get response for 5 simultaneous requests 2.94885587692 Server log looks like: entered function 0.460360050201 in this function entered function 0.459032058716 in this function entered function 0.462526082993 in this function entered function 0.45965385437 in this function entered function 0.460576057434 in this function Each request takes around 0.5 seconds to complete. Plus there is an additional time for request and response to move over the wire. In total it takes around 3 seconds. Time to get response for n requests is increasing linearly as we increase n. With a single worker and single thread, time to get response for simultaneous requests was increasing linearly with number of requests in I/O bound handler too. Let’s change gunicorn threadsfrom its default value of 1 to 5. threads = 5 Let’s make 5 simultaneous requests again. In [33]: make_n_requests(5) Time to get response for 5 simultaneous requests 3.37198781967 We didn’t get any advantage by increasing number of threads. Instead the performance deteriorated. Server log looks like: entered function entered function entered function entered function entered function 2.68351387978 in this function 2.67835998535 in this function 2.67423701286 in this function 2.67234015465 in this function 2.6794462204 in this function Event different requests didn’t complete in 0.5 second as was happening with single thread. The CPU time was split between 5 threads and so instead of 0.5 seconds it took 2.6 seconds for each request to complete. Observation In CPU bound applications, CPU isn’t idle. So multiple threads don’t provide an advantage. CPU time is split between threads, which infact leads to longer response time for each request. Thread switching brings a performance hit. That’s why we saw response time for 5 requests going up from 2.9 seconds to 3.3 seconds for 5 simultaneous requests. Takeaways There is no definite answer for how many workers and threads will provide maximum performance. It’s a matter of tuning and finding out. Making number of workers/threads greater than number of cores almost always leads to higher number of requests per second handling in a I/O bound application. Making number of workers/threads greater than number of cores almost always leads to reduced number of requests per second handling in a CPU bound application. Making number of workers/threads greater than number of cores leads to increase in response time for each request in a CPU bound application." />
<link rel="canonical" href="http://localhost:4000/gunicorn/2017/11/15/how-performant-your-python-web-application.html" />
<meta property="og:url" content="http://localhost:4000/gunicorn/2017/11/15/how-performant-your-python-web-application.html" />
<meta property="og:site_name" content="Agiliq Blogs" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2017-11-15T12:26:01+05:30" />
<script type="application/ld+json">
{"description":"This post tries to explain web application performance. Performance means the number of requests per second that can be served by a deployed application. This post would help answer questions like: How performant is an application. How much load can it handle. How many concurrent requests can it serve. How can you determine requests per second for an application. What steps to take to increase serving capability for an application. This post has as much code as theory. This post assumes that you have a basic understanding of processes, threads. You can read our previous post for basic understanding of processes and threads. In any web application there are many urls and associated handlers/controllers for each url. A url might respond in 200ms while another url might take 3 seconds. While determining performance of an application, choose the url which will be used most often and determine its performance. Factors which determine performance There are many factors determining performance of an application. Major factors are: Application type Application complexity Web server Physical server, i.e infrastructure Web server configuration In this post we will see how changing web server configuration changes performance with other factors remaining constant. Two major components of web server configuration are: Number of processes/workers Number of threads We will also see what web server configuration should be preferred for which application type. Example: How increasing number of server processes boosts performance for some applications while it reduces performance for other application types. Application type Applications could be compute intensive or network intensive or I/O intensive. Compute intensive applications can’t get any benefit by making number of server workers or threads greater than number of CPU cores. Network intensive or I/O intensive applications can benefit a lot by making several server workers/threads run on each core. Application complexity Suppose a url handler makes two db calls and takes a second to respond. In such case reducing number of db calls to 1 will reduce response time by half and number of requests served per second will get doubled. Web server There are many web servers. eg: Apache, Gunicorn, uwsgi etc. Apache might be better than gunicorn and might be able to handle more requests per second than gunicorn. Physical server/Infrastructure Increasing the number of cores or memory will improve the performance. If a single core machine is able to handle 10 requests per second for a computationally intensive application, then a machine with 2 cores should be able to handle 20 requests per second. This might need properly configuring the web server to get maximum utilization from physical server. Application server configuration Number of running web server processes influences performance. Similarly number of server threads in each process influences performance too. Demo server configuration Our demo physical server is a t1.micro instance with 1 GB RAM. It has a single core. Demo application for this post uses Django/Python served from a Gunicorn application server. Familiarity with Django would be helpful but you should be able to follow as long as you have understanding of any web framework. Demo application Our application has the following url: http://34.233.117.92:8000/static_content_sleep The Django handler/controller for url static_content_sleep looks like: def static_content_sleep(request): st = time.time() # Compute Start time print &quot;sleeping&quot; time.sleep(1) # Simulate db call which takes 1 seconds print &quot;waking&quot; print time.time() - st, &quot;in this function&quot; # Compute end time return HttpResponse(&quot;Woke up&quot;) Most web applications would be working with a database to fetch data. We don’t have a database setup. We are assuming that db used by the handler responds in 1 second. We are simulating a db call by making the code execution sleep for 1 second. This application is being served by Gunicorn web server. Gunicorn configuration looks like: workers = 1 threads = 1 bind = &#39;0.0.0.0:8000&#39; daemon = False The important configuration variables are workers and threads. Ignore others. If you want to setup gunicorn on your physical server, you can refer our previous post. We used default gunicorn configuration for workers and threads. The default configuration of gunicorn has following characteristics: workers Default is 1. This means that only one gunicorn process would be running on the physical server. If we make it 2, it would mean that 2 gunicorn processes would be running on the server. threads Default is 1. This tells number of threads in each worker process. This means that each gunicorn worker is single threaded and isn’t multithreaded. Making requests Let’s make two simultaneous request to this url. Making two requests from browser will not be simultaneous as switching from one browser tab to another tab might take you more than a second. In that case you cannot properly observe time taken by the server to process two simultaneous requests. Let’s write a Python function to make simultaneous requests and log the time. In [15]: from threading import Thread In [16]: import requests In [17]: class UrlThread(Thread): ...: def run(self): ...: resp = requests.get(&#39;http://34.233.117.92:8000/static_content_sleep&#39;) In [18]: def make_n_requests(num_requests): ...: threads = [] ...: for i in range(num_requests): ...: threads.append(UrlThread()) ...: start = time.time() # The requests will be almost simultaneous. # Second request will be made within nanoseconds of making the first request. ...: for thread in threads: ...: thread.start() # Threads will be started without waiting for response of previous threads ...: for thread in threads: ...: thread.join() # Wait for response for all the requests. ...: end = time.time() ...: print &quot;Time to get response for %d simultaneous requests&quot; % (num_requests,), end - start You can use curl or a shell script or any programming language using which you can simulate n simultaneous requests. Make 2 simultaneous requests In [19]: make_n_requests(2) Time to get response for 2 simultaneous requests 2.622205019 Gunicorn log on server looks like: sleeping waking 1.00131487846 in this function sleeping waking 1.00130581856 in this function Observation Print statements of the handler are printed. Print statements are printed again. Based on prints, we can infer that second request’s execution started after completion of first request. Let’s make 5 simultaneous requests and see what happens In [20]: make_n_requests(5) Time to get response for 5 simultaneous requests 5.65813708305 Gunicorn log looks like: sleeping waking 1.00114989281 in this function sleeping waking 1.0011370182 in this function sleeping waking 1.00114202499 in this function sleeping waking 1.00130295753 in this function sleeping waking 1.00131988525 in this function You can see 5 sets of print statements. Observation As we increase number of simultaneous requests, response time is proportionately increasing. For 2 requests, server takes around 2 seconds to respond. For 5 requests, server takes around 5 seconds to respond. For 10 requests, it would take around 10 seconds to respond. Let’s change gunicorn threadsfrom its default value of 1 to 5. threads = 5 Gunicorn process will have 5 threads now. And each thread is capable of serving a request. Hence 5 requests can be simultaneosly served. Restart gunicorn. Let’s make 5 simultaneous requests again. In [27]: make_n_requests(5) Time to get response for 5 simultaneous requests 1.77244091034 ** While earlier it was taking around 5 seconds to get 5 simultaneous requests processed, now it takes less than 2 seconds. ** Server log looks like: sleeping sleeping sleeping sleeping sleeping waking 1.001278162 in this function waking 1.00262784958 in this function waking 1.00353288651 in this function waking 1.00325298309 in this function waking 1.0035841465 in this function Important Observation Each gunicorn thread could handle a request. Processor kept switching between all these 5 threads. This way all 5 requests being processed by 5 different threads got a chance to execute concurrently without waiting for completion of any request. Print pattern also suggests that execution of 5 requests started before waiting for completion of first request. We can accomplish this performance boost by increasing number of workers instead of number of threads. We will change threads back to 1 and increase workers to 5 instead. workers = 5 threads = 1 Make 5 simultaneous requests In [22]: make_n_requests(5) Time to get response for 5 simultaneous requests 1.76864600182 Server gunicorn log looks like: sleeping sleeping sleeping sleeping sleeping waking 1.00134015083 in this function waking 1.00125384331 in this function waking 1.00277590752 in this function waking 1.00121593475 in this function waking 1.00092220306 in this function Here, processor executed 5 processes parallely. Let’s make 10 simultaneous requests: In [23]: make_n_requests(10) Time to get response for 10 simultaneous requests 2.74328804016 Server log looks like: sleeping -&gt; First 5 requests are assigned to 5 workers sleeping sleeping sleeping sleeping waking -&gt; One worker, say worker 1, finished execution. waking -&gt; Another worker, say worker 2, finished execution. This print was executed before last print statement of worker 1 could execute. 1.00396203995 in this function -&gt; Worker 1 last print statement. Worker 1 free now and can serve another request. 1.00669813156 in this function -&gt; Worker 2 last print statement. Worker 2 free now sleeping -&gt; Assigned to probably worker 1 waking -&gt; Probably worker 3 woke up. 1.00510120392 in this function sleeping waking 1.00605106354 in this function sleeping sleeping waking 1.00563693047 in this function sleeping waking 1.00345993042 in this function waking 1.00495409966 in this function waking 1.00393104553 in this function waking 1.0027141571 in this function waking 1.00151586533 in this function Observation We made 10 simultaneous requests. First 5 requests were assigned to 5 running gunicorn processes. First 5 requests were concurrently handled during 1st second. 5 remaining requests were waiting to be executed during this time. During next second each gunicorn process picked up another request after completing a request. That’s why it took around 2 seconds to get response for 10 requests. Let’s use 2 threads with 5 workers threads = 2 workers = 5 Let’s restart gunicorn. Now there are 5 gunicorn processes running and each process is running 2 threads. In [16]: make_n_requests(10) Time to get response for 10 simultaneous requests 1.46275486946 Server log looks like: sleeping -&gt; &#39;sleeping&#39; repeated 10 times before any &#39;waking&#39;. sleeping sleeping sleeping sleeping sleeping sleeping sleeping sleeping sleeping waking 1.00119280815 in this function waking 1.00499987602 in this function waking 1.00233006477 in this function waking 1.00144195557 in this function waking 1.00114202499 in this function waking 1.00117397308 in this function waking 1.00114512444 in this function waking 1.00120997429 in this function waking 1.00124192238 in this function waking 1.00117397308 in this function Inference Based on these examples we can infer that making number of threads/workers greater than number of cores improves the performance for a network intensive and I/O intensive application. Can we keep increasing workers and threads You have to keep RAM usage under consideration while tuning the number of workers and threads. Code execution for a handler needs memory. While worker is processing a request, sufficient memory must be available. If handling each request needs 50 MB and you have 5 workers and 1 thread running, you must ensure that 250 MB free RAM is there after starting gunicorn. Performance with current configuration Curent configuration has 2 workers and 5 threads for each worker. So 10 requests will be handled concurrently. Each request takes around 1 second to respond. So with current configuration server can handle 10 requests per second. If the handler is changed to time.sleep(0.5), i.e if each request could respond in approximately 0.5 seconds then server would be able to handle 20 requests per second. If we change number of workers to 3 and 5 threads for each worker, then with time.sleep(0.5), server would be able to handle 3*5*2, i,e 30 requests per second. Increase performance by increasing number of cores Suppose we use a machine with 2 cores. On single core machine if we were using 2 workers, then on double core machine we should use 4 workers. This assumes that there is sufficient RAM available to be allocated to 4 workers. A dual core machine would be able to handle 4*5, i.e 20 requests per second, assuming each request responds in a second. If we found that 3 workers with 5 thread each is an optimum combination on a single core machine, then we should use 6 workers on a dual core machine. Computationally bound applications Earlier sections discussed about I/O bound applications. Let’s talk about CPU bound applications. App has a following url: http://34.233.117.92:8000/list_of_dict The Django handler/controller for url list_of_dict looks like: def list_of_dict(request): print &quot;entered function&quot; st = time.time() for i in xrange(30000000): pass print time.time() - st, &quot;in this function&quot; return HttpResponse(json.dumps(&quot;a&quot;)) Let’s change UrlThread used by make_n_requests to work with this new url. In [14]: class UrlThread(Thread): ...: def run(self): ...: resp = requests.get(&#39;http://34.233.117.92:8000/list_of_dict&#39;) Let’s change gunicorn configuration to have a single worker and single thread. workers = 1 threads = 1 Let’s make 1 request to this url In [28]: make_n_requests(1) Time to get response for 1 simultaneous requests 1.49780297279 Server log looks like: entered function 0.454895019531 in this function It takes around 0.5 seconds in the handler as you can see from server log. And it takes around 1 second for request and response to travel on the wire. So in total it takes around 1.5 seconds. Let’s make 5 requests to this url In [32]: make_n_requests(5) Time to get response for 5 simultaneous requests 2.94885587692 Server log looks like: entered function 0.460360050201 in this function entered function 0.459032058716 in this function entered function 0.462526082993 in this function entered function 0.45965385437 in this function entered function 0.460576057434 in this function Each request takes around 0.5 seconds to complete. Plus there is an additional time for request and response to move over the wire. In total it takes around 3 seconds. Time to get response for n requests is increasing linearly as we increase n. With a single worker and single thread, time to get response for simultaneous requests was increasing linearly with number of requests in I/O bound handler too. Let’s change gunicorn threadsfrom its default value of 1 to 5. threads = 5 Let’s make 5 simultaneous requests again. In [33]: make_n_requests(5) Time to get response for 5 simultaneous requests 3.37198781967 We didn’t get any advantage by increasing number of threads. Instead the performance deteriorated. Server log looks like: entered function entered function entered function entered function entered function 2.68351387978 in this function 2.67835998535 in this function 2.67423701286 in this function 2.67234015465 in this function 2.6794462204 in this function Event different requests didn’t complete in 0.5 second as was happening with single thread. The CPU time was split between 5 threads and so instead of 0.5 seconds it took 2.6 seconds for each request to complete. Observation In CPU bound applications, CPU isn’t idle. So multiple threads don’t provide an advantage. CPU time is split between threads, which infact leads to longer response time for each request. Thread switching brings a performance hit. That’s why we saw response time for 5 requests going up from 2.9 seconds to 3.3 seconds for 5 simultaneous requests. Takeaways There is no definite answer for how many workers and threads will provide maximum performance. It’s a matter of tuning and finding out. Making number of workers/threads greater than number of cores almost always leads to higher number of requests per second handling in a I/O bound application. Making number of workers/threads greater than number of cores almost always leads to reduced number of requests per second handling in a CPU bound application. Making number of workers/threads greater than number of cores leads to increase in response time for each request in a CPU bound application.","author":{"@type":"Person","name":"akshar"},"@type":"BlogPosting","url":"http://localhost:4000/gunicorn/2017/11/15/how-performant-your-python-web-application.html","headline":"How performant is your Python web application","dateModified":"2017-11-15T12:26:01+05:30","datePublished":"2017-11-15T12:26:01+05:30","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/gunicorn/2017/11/15/how-performant-your-python-web-application.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js"></script>
    <script src="/assets/js/respond.js"></script>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
    <!--[if lt IE 9]>
      <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <!--[if lt IE 8]>
    <link rel="stylesheet" href="/assets/css/ie.css">
    <![endif]-->
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <script>
    	function onPageLoad() {
    		var element = document.getElementById("content");
    		var childNodes = element.childElementCount;
    		if(childNodes > 0) {
    		 	element.scrollIntoView(); 
    	 	}

    	}
    </script>
    
  </head>
  <body onload="onPageLoad()"> 
      <div id="header">
        <nav>
          <li class="fork"><a href="">View On GitHub</a></li>
          <!-- 
            <li class="downloads"><a href="">ZIP</a></li>
            <li class="downloads"><a href="">TAR</a></li>
            <li class="title">DOWNLOADS</li>
           -->
        </nav>
      </div><!-- end header -->

    <div class="wrapper">

      <section>
        <div id="title">
          <h1>Agiliq Blogs</h1>
          <!-- <p>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.</p> -->
          <hr>
          <span class="credits left">Project maintained by <a href="">Agiliq</a></span>
          <span class="credits right">Hosted on GitHub Pages &mdash; Theme by <a href="https://twitter.com/michigangraham">mattgraham</a></span>
          <ul>
		  
		  <li>
		    <a href="/python/2018/01/21/prime-number-binary-trex.html" >Finding a prime number whose binary representation is a giraffe (or a T-Rex)</a>
		  </li>
		  
		  <li>
		    <a href="/django/2017/12/29/when-and-how-use-django-listview.html" >When and how to use Django ListView</a>
		  </li>
		  
		  <li>
		    <a href="/django/2017/12/29/when-and-how-use-django-templateview.html" >When and how to use Django TemplateView</a>
		  </li>
		  
		  <li>
		    <a href="/django/2017/12/11/adventures-in-advanced-django-orm-with-hyperloglog.html" >Adventures in advanced Django ORM with HyperLogLog</a>
		  </li>
		  
		  <li>
		    <a href="/django/2017/12/06/django-20-window-expressions-tutorial.html" >Django 2.0 Window expressions tutorial</a>
		  </li>
		  
		  <li>
		    <a href="/django/2017/12/02/configure-django-log-exceptions-production.html" >Configure Django to log exceptions in production</a>
		  </li>
		  
		  <li>
		    <a href="/gunicorn/2017/11/15/how-performant-your-python-web-application.html" >How performant is your Python web application</a>
		  </li>
		  
		  <li>
		    <a href="/python/2017/11/01/how-python-generators-are-similar-iterators.html" >How Python generators are similar to iterators</a>
		  </li>
		  
		  <li>
		    <a href="/python/2017/10/18/real-world-usage-iterators-and-iterables.html" >Real world usage of __iter__ and next</a>
		  </li>
		  
		  <li>
		    <a href="/python/2017/10/12/iterators-and-iterables.html" >Iterators and Iterables</a>
		  </li>
		  
		  <li>
		    <a href="/appengine/2017/09/04/getting-started-webapp2-and-gae.html" >Getting started with webapp2 and GAE</a>
		  </li>
		  
		  <li>
		    <a href="/angularjs/2017/04/30/why-angularjs-services-arent-available-configurati.html" >Why AngularJS services aren't available in configuration blocks</a>
		  </li>
		  
		  <li>
		    <a href="/angularjs/2017/04/27/what-when-and-how-angularjs-configuration-blocks.html" >What, when and how of AngularJS configuration blocks</a>
		  </li>
		  
		  <li>
		    <a href="/2017/04/26/trsfer-files-amzon-s3-using-browser-instead-server.html" >Transfer files to amazon s3 using browser instead of server</a>
		  </li>
		  
		  <li>
		    <a href="/angularjs/2017/04/25/angularjs-injectors-internals.html" >AngularJS injectors internals</a>
		  </li>
		  
		  <li>
		    <a href="/angularjs/2017/04/21/how-script-ordering-works-angular-app.html" >How script ordering works in an Angular app</a>
		  </li>
		  
		  <li>
		    <a href="/scrapy/2016/04/02/getting-started-with-python-scrapy.html" >Getting started with python scrapy</a>
		  </li>
		  
		  <li>
		    <a href="/celery/2015/08/03/retrying-celery-failed-tasks.html" >Retrying celery failed tasks</a>
		  </li>
		  
		  <li>
		    <a href="/middlewares/2015/07/17/profiling-django-middlewares.html" >Profiling Django Middlewares</a>
		  </li>
		  
		  <li>
		    <a href="/middlewares/2015/07/17/understanding-django-middlewares.html" >Understanding Django Middlewares</a>
		  </li>
		  
		  <li>
		    <a href="/redis/2015/07/06/getting-started-with-celery-and-redis.html" >Getting started with Celery and Redis</a>
		  </li>
		  
		  <li>
		    <a href="/django-tastypie/2015/03/29/tastypie-with-foreignkey.html" >Tastypie with ForeignKey</a>
		  </li>
		  
		  <li>
		    <a href="/redis/2015/03/26/getting-started-with-redis-py.html" >Getting started with redis-py</a>
		  </li>
		  
		  <li>
		    <a href="/django-tastypie/2015/03/23/getting-started-with-django-tastypie.html" >Getting started with Django tastypie</a>
		  </li>
		  
		  <li>
		    <a href="/google/2015/03/04/building-chrome-extensions.html" >Building Chrome Extensions</a>
		  </li>
		  
		  <li>
		    <a href="/disqus/2015/01/16/importing-your-old-comments-to-disqus-site.html" >Importing your old comments to Disqus site</a>
		  </li>
		  
		  <li>
		    <a href="/python/2014/12/08/how-not-knowing-encoding-can-trip-you.html" >How not knowing encoding can trip you</a>
		  </li>
		  
		  <li>
		    <a href="/python/2014/12/08/understanding-python-unicode-str-unicodeencodeerro.html" >Understanding Python unicode, str, UnicodeEncodeError and UnicodeDecodeError</a>
		  </li>
		  
		  <li>
		    <a href="/api/2014/12/04/building-a-restful-api-with-django-rest-framework.html" >Building a RESTful API with Django-rest-framework</a>
		  </li>
		  
		  <li>
		    <a href="/encoding/2014/11/19/character-encoding-and-unicode.html" >Character encoding and Unicode</a>
		  </li>
		  
		  <li>
		    <a href="/disqus/2014/11/17/disqus-and-disqus-sso.html" >Disqus and Disqus SSO</a>
		  </li>
		  
		  <li>
		    <a href="/http/client/2014/09/08/using-postman.html" >Using a Postman http client for efficient HTTP testing</a>
		  </li>
		  
		  <li>
		    <a href="/functional-testing/2014/09/02/advanced-functional-testing-with-selenium.html" >Advanced functional testing with Selenium in Django</a>
		  </li>
		  
		  <li>
		    <a href="/functional-testing/2014/09/01/selenium-testing.html" >Introduction to functional testing with Selenium in Django</a>
		  </li>
		  
		  <li>
		    <a href="/coveralls.io/2014/08/22/travis-and-coveralls-for-private-repo.html" >Travis and coveralls for private repo</a>
		  </li>
		  
		  <li>
		    <a href="/django/2014/08/20/django-timezones.html" >Django timezones</a>
		  </li>
		  
		  <li>
		    <a href="/django/app/deployment/2014/08/06/deploying-a-django-app-on-amazon-ec2-instance.html" >Deploying a Django app on Amazon EC2 instance.</a>
		  </li>
		  
		  <li>
		    <a href="/django/2014/08/05/passing-parameters-to-django-admin-action.html" >Passing parameters to Django admin action</a>
		  </li>
		  
		  <li>
		    <a href="/python/2014/07/15/method-decorators-in-python.html" >Method decorators in Python</a>
		  </li>
		  
		  <li>
		    <a href="/gunicorn/2014/06/05/minimal-gunicorn-configuration.html" >Minimal Gunicorn configuration</a>
		  </li>
		  
		  <li>
		    <a href="/heroku/2014/06/05/heroku-django-s3-for-serving-media-files.html" >Heroku Django S3 for serving Media files</a>
		  </li>
		  
		  <li>
		    <a href="/mysqltopostgres/2014/05/27/migrating-django-app-from-mysql-to-postgres.html" >Migrating django app from MySQL to Postgres</a>
		  </li>
		  
		  <li>
		    <a href="/google/2014/05/09/google-diff-match-patch-library.html" >Google diff match patch library</a>
		  </li>
		  
		  <li>
		    <a href="/supervisor/2014/05/09/supervisor-with-django-and-gunicorn.html" >Supervisor with Django and Gunicorn</a>
		  </li>
		  
		  <li>
		    <a href="/python/2014/05/05/python-requests.html" >Python-requests</a>
		  </li>
		  
		  <li>
		    <a href="/python/2014/05/01/three-underutilized-python-commands.html" >Three underutilized python commands</a>
		  </li>
		  
		  <li>
		    <a href="/travis/2014/05/01/continuous-integration-with-travis-and-coverallsio.html" >Continuous integration with travis and coveralls.io for Django apps</a>
		  </li>
		  
		  <li>
		    <a href="/django/2014/04/28/django-backward-relationship-lookup.html" >Django backward relationship lookup</a>
		  </li>
		  
		  <li>
		    <a href="/threads/2013/10/17/producer-consumer-problem-in-python.html" >Producer-consumer problem in Python</a>
		  </li>
		  
		  <li>
		    <a href="/design/pattern/2013/10/14/state-pattern-with-ui-code.html" >State pattern with UI Code</a>
		  </li>
		  
		  <li>
		    <a href="/threads/2013/09/17/understanding-threads-in-python.html" >Understanding Threads in Python</a>
		  </li>
		  
		  <li>
		    <a href="/process/2013/09/11/process-and-threads-for-beginners.html" >Process and Threads for Beginners</a>
		  </li>
		  
		  <li>
		    <a href="/nginx/2013/08/26/minimal-nginx-and-gunicorn-configuration-for-djang.html" >Minimal Nginx and Gunicorn configuration for Django projects</a>
		  </li>
		  
		  <li>
		    <a href="/threads/2013/08/21/writing-thread-safe-django-code.html" >Writing thread-safe django - get_or_create</a>
		  </li>
		  
		  <li>
		    <a href="/python/2013/07/12/accept-bitcoins-using-python.html" >Accept bitcoins using python</a>
		  </li>
		  
		  <li>
		    <a href="/python/2013/07/01/basics-wsgi.html" >Basics of WSGI</a>
		  </li>
		  
		  <li>
		    <a href="/docker/2013/06/28/self-testing-fabfile-using-docker.html" >Self-testing fabfile using docker</a>
		  </li>
		  
		  <li>
		    <a href="/docker/2013/06/14/deploying-django-using-docker.html" >Deploying django using docker</a>
		  </li>
		  
		  <li>
		    <a href="/testing/2013/04/28/common-testing-scenarios-for-django-app.html" >Common testing scenarios for Django app.</a>
		  </li>
		  
		  <li>
		    <a href="/static/2013/03/21/serving-static-files-in-django.html" >Serving static files in Django</a>
		  </li>
		  
		  <li>
		    <a href="/book/2013/02/11/two-scoops-of-django-review.html" >Two Scoops of Django: Review</a>
		  </li>
		  
		  <li>
		    <a href="/training/2013/02/08/introduction-to-python-workshop-on-february-15th-2.html" >Introduction to Python Workshop on February 15th, 2013</a>
		  </li>
		  
		  <li>
		    <a href="/django/2013/02/07/easy-client-side-form-validations-for-django-djang.html" >Easy client side form validations for Django: Django Parsley</a>
		  </li>
		  
		  <li>
		    <a href="/open-source/2013/01/21/moreapps-android-library-project-open-sourced.html" >MoreApps - Android Library Project: Open Sourced</a>
		  </li>
		  
		  <li>
		    <a href="/open-source/2013/01/15/password-generetor-app-open-sourced.html" >Password Generator App: Open Sourced</a>
		  </li>
		  
		  <li>
		    <a href="/open-source/2013/01/14/todo-list-app-open-sourced.html" >Todo List App: Open Sourced</a>
		  </li>
		  
		  <li>
		    <a href="/android/2013/01/01/android-fragments-101.html" >Android Fragments 101</a>
		  </li>
		  
		  <li>
		    <a href="/function/as/objects/2012/11/17/understanding-decorators-2.html" >Understanding decorators</a>
		  </li>
		  
		  <li>
		    <a href="/forms/2012/11/07/not-exactly-not-exactly-tim-the-enchanter.html" >Not exactly, not exactly tim the enchanter</a>
		  </li>
		  
		  <li>
		    <a href="/internals/2012/11/01/the-missing-documentation-for-djangoutilsdatastruc.html" >The missing documentation for django.utils.datastructures</a>
		  </li>
		  
		  <li>
		    <a href="/2012/09/06/dissecting-phonegaps-architecture.html" >Dissecting Phonegaps Architecture</a>
		  </li>
		  
		  <li>
		    <a href="/upload/2012/07/17/dropbox-file-upload-handler-for-django.html" >Dropbox file upload handler for django</a>
		  </li>
		  
		  <li>
		    <a href="/virtualization/2012/07/16/using-ubuntu-cloud-images-in-kvm.html" >Using Ubuntu cloud images in KVM</a>
		  </li>
		  
		  <li>
		    <a href="/metaclass/2012/07/02/metaclass-python.html" >Metaclass in Python</a>
		  </li>
		  
		  <li>
		    <a href="/virtualization/2012/06/25/libvirt-and-kvm.html" >Libvirt and KVM</a>
		  </li>
		  
		  <li>
		    <a href="/__new__/2012/06/10/__new__-python.html" >__new__() in python</a>
		  </li>
		  
		  <li>
		    <a href="/kwargs/2012/06/03/understanding-args-and-kwargs.html" >Understanding '*', '*args', '**' and '**kwargs'</a>
		  </li>
		  
		  <li>
		    <a href="/provisioning/2012/05/29/provisioning-made-easy-with-chef.html" >Provisioning Made Easy With Chef</a>
		  </li>
		  
		  <li>
		    <a href="/development/2012/05/02/test-driven-development-python.html" >Test Driven Development in Python </a>
		  </li>
		  
		  <li>
		    <a href="/terminal/2012/03/20/developing-android-applications-from-command-line.html" >Developing android applications from command line</a>
		  </li>
		  
		  <li>
		    <a href="/vps/2012/02/23/deploy-django-app-5-easy-steps.html" >Deploy Django App in 5 Easy Steps</a>
		  </li>
		  
		  <li>
		    <a href="/django/2012/02/22/deploying-django-apps-on-heroku.html" >Deploying Django apps on Heroku</a>
		  </li>
		  
		  <li>
		    <a href="/site_id/prefix/2012/02/05/dynamically-attaching-site_id-django-caching.html" > Dynamically attaching SITE_ID to Django Caching</a>
		  </li>
		  
		  <li>
		    <a href="/django/2012/02/04/deploying-django-apps-on-heroku-2.html" >Deploying Django apps on Heroku</a>
		  </li>
		  
		  <li>
		    <a href="/screencasts/2012/02/03/how-to-use-pep8py-to-write-better-django-code.html" >How to use pep8.py to write better Django code</a>
		  </li>
		  
		  <li>
		    <a href="/screencasts/2012/02/02/how-and-why-to-use-pyflakes-to-write-better-python.html" >How and why to use pyflakes to write better Python</a>
		  </li>
		  
		  <li>
		    <a href="/south/2012/01/09/south.html" >Getting started with South for Django DB migrations</a>
		  </li>
		  
		  <li>
		    <a href="/coffeescript/2012/01/06/writing-jquery-plugins-using-coffeescript.html" >Writing jQuery plugins using Coffeescript</a>
		  </li>
		  
		  <li>
		    <a href="/response/2012/01/02/behind-the-scenes-request-to-response.html" >Request to Response</a>
		  </li>
		  
		  <li>
		    <a href="/java/2011/12/28/using-sqlite-database-with-android.html" >Using SQLite Database with Android</a>
		  </li>
		  
		  <li>
		    <a href="/django/2011/12/25/haml-for-django-developers.html" >Haml for Django developers</a>
		  </li>
		  
		  <li>
		    <a href="/coffeescript/2011/12/24/coffeescript-for-python-programmers.html" >Coffeescript for Python programmers</a>
		  </li>
		  
		  <li>
		    <a href="/treeview/2011/10/12/how-use-jstree.html" >How to use jsTree</a>
		  </li>
		  
		  <li>
		    <a href="/upload/2011/09/21/behind-the-scenes-from-html-form-to-storage.html" >From HTML Form to Storage</a>
		  </li>
		  
		  <li>
		    <a href="/mysql/2011/07/01/setting-your-system-start-django-development-ubunt.html" >Setting up your system to start with Django development on Ubuntu:</a>
		  </li>
		  
		  <li>
		    <a href="/e-mail/2011/04/05/writing-an-e-mail-application-with-lamson-ii.html" >Writing an e-mail application with Lamson - II</a>
		  </li>
		  
		  <li>
		    <a href="/e-mail/2011/04/01/writing-an-e-mail-application-with-lamson-i.html" >Writing an e-mail application with Lamson - I</a>
		  </li>
		  
		  <li>
		    <a href="/jobs/2011/03/15/jobs.html" >Jobs</a>
		  </li>
		  
		  <li>
		    <a href="/java/2011/02/08/comparision-iphone-android-phonegap-titanium.html" >Comparison of mobile app frameworks: Iphone, Java, Phonegap and Titanium</a>
		  </li>
		  
		  <li>
		    <a href="/iphone/2011/02/08/getting-started-with-titanium-development.html" >Getting started with Titanium development for Android and Iphone</a>
		  </li>
		  
		  <li>
		    <a href="/android/2011/02/06/getting-started-with-phonegap-using-xcode-for-mobi.html" >Getting started with PhoneGap using Xcode for Mobile app development</a>
		  </li>
		  
		  <li>
		    <a href="/java/2011/02/06/starting-android-app-developement-from-zero-to-app.html" >Starting Android app developement: From zero to app</a>
		  </li>
		  
		  <li>
		    <a href="/mobile/applications/2011/02/03/iphoneandroid-application-development-using-titani.html" >iPhone and Android application development using Titanium</a>
		  </li>
		  
		  <li>
		    <a href="/linkroundup/2011/01/21/link-roundup-10.html" >Link roundup 10</a>
		  </li>
		  
		  <li>
		    <a href="/linkroundup/2011/01/07/link-roundup-9.html" >Link roundup 9</a>
		  </li>
		  
		  <li>
		    <a href="/reviews/2010/12/31/book-review-the-principles-of-beautiful-web-design.html" >Book Review: The Principles Of Beautiful Web Design</a>
		  </li>
		  
		  <li>
		    <a href="/linkroundup/2010/12/31/link-roundup-8.html" >Link roundup 8</a>
		  </li>
		  
		  <li>
		    <a href="/emacs/2010/12/27/django-emacs-setup.html" >Django emacs setup</a>
		  </li>
		  
		  <li>
		    <a href="/reviews/2010/12/26/book-review-pragmatic-guide-to-javascript.html" >Book Review: Pragmatic Guide to JavaScript</a>
		  </li>
		  
		  <li>
		    <a href="/reviews/2010/12/25/the-principles-of-successful-freelancing.html" >Book review: The Principles of Successful Freelancing</a>
		  </li>
		  
		  <li>
		    <a href="/reviews/2010/12/24/book-review-the-principles-of-project-management.html" >Book review: The Principles of Project Management</a>
		  </li>
		  
		  <li>
		    <a href="/reviews/2010/12/24/book-review-outsourcing-web-projects.html" >Book Review: Outsourcing Web Projects.</a>
		  </li>
		  
		  <li>
		    <a href="/linkroundup/2010/12/23/link-roundup-for-week-ending-24122010.html" >Link roundup for week ending 24/12/2010</a>
		  </li>
		  
		  <li>
		    <a href="/%0A2010-12-22-real-time-applications-with-django-xmpp-and-stroph.markdown/xmpp/2010/12/22/real-time-applications-with-django-xmpp-and-stroph.html" >Real time applications with Django, XMPP and StropheJS</a>
		  </li>
		  
		  <li>
		    <a href="/tips/2010/12/04/the-unfuddle-tutorial.html" >The Unfuddle Tutorial</a>
		  </li>
		  
		  <li>
		    <a href="/apps/2010/12/04/experiments-in-url-design.html" >Experiments in URL design.</a>
		  </li>
		  
		  <li>
		    <a href="/linkroundup/2010/11/26/link-roundup-for-week-ending-26112010.html" >Link roundup for week ending 26/11/2010</a>
		  </li>
		  
		  <li>
		    <a href="/linkroundup/2010/11/19/link-roundup-for-week-ending-19112010.html" >Link Roundup for week ending 19/11/2010</a>
		  </li>
		  
		  <li>
		    <a href="/linkroundup/2010/11/12/link-roundup-for-week-ending-12112010.html" >Link Roundup for week ending 12/11/2010</a>
		  </li>
		  
		  <li>
		    <a href="/python/2010/11/07/i-am-so-starving-same-web-app-in-various-python-we.html" >I am so starving: Web app in python frameworks.</a>
		  </li>
		  
		  <li>
		    <a href="/vim/2010/11/03/seven-reasons-why-you-should-switch-to-vim-for-dja.html" >Seven reasons why you should switch to Vim</a>
		  </li>
		  
		  <li>
		    <a href="/linkroundup/2010/11/03/link-roundup-for-week-ending-5112011.html" >Link Roundup for week ending 5/11/2011</a>
		  </li>
		  
		  <li>
		    <a href="/wordpress/2010/10/28/importing-wordpress.html" >Importing wordpress</a>
		  </li>
		  
		  <li>
		    <a href="/api/2010/10/25/getting-trending-github-projects-via.html" >Getting trending Github projects via YQL</a>
		  </li>
		  
		  <li>
		    <a href="/linkroundup/2010/10/22/link-roundup-for-week-ending-2.html" >Link Roundup for week ending 22/10/2010</a>
		  </li>
		  
		  <li>
		    <a href="/business/2010/10/18/essential-web-apps-to-run-a.html" >Essential web-apps to run a software business.</a>
		  </li>
		  
		  <li>
		    <a href="/opinion/2010/10/17/django-is-not-flexible.html" >Django is not flexible</a>
		  </li>
		  
		  <li>
		    <a href="/linkroundup/2010/10/15/link-roundup-for-week-ending.html" >Link Roundup for week ending 15/10/2010</a>
		  </li>
		  
		  <li>
		    <a href="/about/2010/06/08/moving-home.html" >Moving home</a>
		  </li>
		  
		  <li>
		    <a href="/uncategorized/2010/03/20/rails-and-django-commands-comparison-and-conversio.html" >Rails and Django commands : comparison  and conversion</a>
		  </li>
		  
		  <li>
		    <a href="/rails/2010/03/20/the-rails-and-django-models-layer-rosseta-stone.html" >The Rails and Django models layer Rosseta stone</a>
		  </li>
		  
		  <li>
		    <a href="/models/2010/01/18/doing-things-with-django-models-aka-django-models.html" >Doing things with Django models - aka - Django models tutorial</a>
		  </li>
		  
		  <li>
		    <a href="/about/2010/01/17/wordpress-and-django-best-buddies.html" >Wordpress and Django: best buddies</a>
		  </li>
		  
		  <li>
		    <a href="/tutorial/2010/01/14/doing-things-with-django-forms.html" >Doing things with Django forms</a>
		  </li>
		  
		  <li>
		    <a href="/apps/2010/01/07/django-forum.html" >django-forum</a>
		  </li>
		  
		  <li>
		    <a href="/uncategorized/2009/12/13/django-buzz.html" >Django-buzz</a>
		  </li>
		  
		  <li>
		    <a href="/tutorial/2009/12/12/using-bpython-shell-with-django-and-some-ipython-f.html" >Using bpython shell with django (and some Ipython features you should know)</a>
		  </li>
		  
		  <li>
		    <a href="/india/2009/12/10/fossin-2009-the-best-fossin-ever.html" >Foss.in 2009: The best foss.in. Ever. </a>
		  </li>
		  
		  <li>
		    <a href="/tutorial/2009/12/03/python-metaclasses-and-how-django-uses-them.html" >Python metaclasses and how Django uses them</a>
		  </li>
		  
		  <li>
		    <a href="/django/2009/12/03/django-quiz.html" >Django quiz</a>
		  </li>
		  
		  <li>
		    <a href="/python/2009/11/26/django-for-a-rails-developer.html" >Django for a Rails Developer</a>
		  </li>
		  
		  <li>
		    <a href="/uncategorized/2009/11/21/the-magic-of-metaclasses-in-python.html" >The magic of metaclasses in Python</a>
		  </li>
		  
		  <li>
		    <a href="/tutorial/2009/11/21/writing-your-own-template-loaders.html" >Writing your own template loaders</a>
		  </li>
		  
		  <li>
		    <a href="/tutorial/2009/11/11/django-gotchas.html" >Django gotchas </a>
		  </li>
		  
		  <li>
		    <a href="/uncategorized/2009/10/02/pycon-india-2009-a-review.html" >Pycon India 2009 : A Review</a>
		  </li>
		  
		  <li>
		    <a href="/tutorial/2009/09/18/beginning-python.html" >Beginning python</a>
		  </li>
		  
		  <li>
		    <a href="/yahoo/2009/08/27/django-socialauth-login-via-twitter-facebook-openi.html" >Django-SocialAuth - Login via twitter, facebook, openid, yahoo, google using a single app.</a>
		  </li>
		  
		  <li>
		    <a href="/rambling/2009/08/20/a-response-to-dropping-django.html" >A response to Dropping Django</a>
		  </li>
		  
		  <li>
		    <a href="/aggreagtion/2009/08/18/django-aggregation-tutorial.html" >Django aggregation tutorial</a>
		  </li>
		  
		  <li>
		    <a href="/tips/2009/07/16/on-captcha.html" >On Captcha</a>
		  </li>
		  
		  <li>
		    <a href="/book/2009/07/03/django-design-patterns.html" >Django design patterns</a>
		  </li>
		  
		  <li>
		    <a href="/django/2009/07/02/remote-debugging-debugging-pesky-server-only-bugs.html" >Remote debugging - debugging pesky server only bugs</a>
		  </li>
		  
		  <li>
		    <a href="/django/2009/06/25/django-request-response-processing.html" >Django Request Response processing</a>
		  </li>
		  
		  <li>
		    <a href="/python/2009/06/24/better-python-package-management-using-source-and.html" >Better Python package management using source and version control systems</a>
		  </li>
		  
		  <li>
		    <a href="/python/2009/06/23/understanding-decorators.html" >Understanding decorators</a>
		  </li>
		  
		  <li>
		    <a href="/algorithms/2009/06/16/generating-pseudo-random-text-with-markov-chains-u.html" >Generating pseudo random text with Markov chains using Python</a>
		  </li>
		  
		  <li>
		    <a href="/yahoo/2009/06/14/yahoo-boss-python-api.html" >Yahoo BOSS python api</a>
		  </li>
		  
		  <li>
		    <a href="/api/2009/06/10/python-wrapper-on-bing-api.html" >Python Wrapper on Bing API</a>
		  </li>
		  
		  <li>
		    <a href="/ecommerce/2009/03/31/exploring-authorizenet-payment-gateway-options-and.html" >Exploring Authorize.net Payment Gateway Options and integrating it with django</a>
		  </li>
		  
		  <li>
		    <a href="/satchmo/2009/03/26/create-your-own-online-store-in-few-hours-using-sa.html" >Create your own online store in few hours using satchmo (django)</a>
		  </li>
		  
		  <li>
		    <a href="/pinax/2009/03/18/create-a-new-social-networking-site-in-few-hours-u.html" >Create a new social networking site in few hours using pinax platform (django).</a>
		  </li>
		  
		  <li>
		    <a href="/uncategorized/2009/03/11/uswaretech-whitepapers.html" >Uswaretech whitepapers</a>
		  </li>
		  
		  <li>
		    <a href="/products/2009/03/11/django-subdomains-easily-create-subscription-based.html" >Django-subdomains - Easily create subscription based subdomains enabled webapps</a>
		  </li>
		  
		  <li>
		    <a href="/algorithms/2009/03/09/finding-keywords-using-python.html" >Finding keywords using Python</a>
		  </li>
		  
		  <li>
		    <a href="/business/2009/03/09/web-development-companies-working-with-django.html" >Web development companies working with Django</a>
		  </li>
		  
		  <li>
		    <a href="/presentations/2009/03/08/developing-a-web-application-live-in-15-min-in-dja.html" > Developing a Web Application Live in 15 min, in django framework</a>
		  </li>
		  
		  <li>
		    <a href="/algorithms/2009/03/06/constraint-programming-in-python.html" >Constraint programming in Python</a>
		  </li>
		  
		  <li>
		    <a href="/mysql/2009/03/06/django-with-mysql-and-apache-on-ec2.html" >Django with Mysql and Apache on EC2</a>
		  </li>
		  
		  <li>
		    <a href="/facebook/2009/02/20/how-to-build-a-facebook-app-in-django.html" >How to build a Facebook app in Django</a>
		  </li>
		  
		  <li>
		    <a href="/web2.0/2009/02/19/how-we-built-a-twitter-application.html" >How we built a Twitter Application</a>
		  </li>
		  
		  <li>
		    <a href="/paypal/2008/11/12/using-paypal-with-django.html" >Using Paypal with Django</a>
		  </li>
		  
		  <li>
		    <a href="/tips/2008/10/10/using-subdomains-with-django.html" >Using subdomains with Django</a>
		  </li>
		  
		  <li>
		    <a href="/forms/2008/10/10/dynamic-forms-with-django.html" >Dynamic forms with Django</a>
		  </li>
		  
		  <li>
		    <a href="/tips/2008/10/07/generating-pdfs-with-django.html" >Generating PDFs with Django </a>
		  </li>
		  
		  <li>
		    <a href="/interviews/2008/06/24/an-interview-with-adrian-holovaty-creator-of-djang.html" >An Interview with Adrian Holovaty - Creator of Django</a>
		  </li>
		  
		  <li>
		    <a href="/python/2008/05/30/an-interview-with-jacob-kaplan-moss-creator-of-dja.html" >An Interview with Jacob Kaplan-Moss - Creator of Django</a>
		  </li>
		  
		  <li>
		    <a href="/startup/2008/05/14/an-idea-a-day-a-geographical-wiki.html" >An idea a day - A geographical wiki</a>
		  </li>
		  
		  <li>
		    <a href="/startup/2008/05/13/an-idea-a-day-alternative-to-gae.html" >An idea a day - Alternative to GAE</a>
		  </li>
		  
		  <li>
		    <a href="/startup/2008/05/12/an-idea-a-day-remotely-hosted-analytics-solution.html" >An idea a day - Remotely hosted Analytics solution</a>
		  </li>
		  
		  <li>
		    <a href="/uncategorized/2008/05/12/popularising-django-part-2.html" >Popularising Django - Part 2</a>
		  </li>
		  
		  <li>
		    <a href="/startup/2008/05/11/an-idea-a-day-recomendation-system-based-ad-networ.html" >An idea a day - Recomendation system based ad network</a>
		  </li>
		  
		  <li>
		    <a href="/startup/2008/05/10/an-idea-a-day-an-automated-adwords-optimizer.html" >An idea a day - An automated Adwords optimizer</a>
		  </li>
		  
		  <li>
		    <a href="/search/2008/05/08/parable-of-the-single-sheep-or-how-google-is-destr.html" >Parable of the single sheep - Or How Google is destroying the internet, and nobody seems to know.</a>
		  </li>
		  
		  <li>
		    <a href="/marketing/2008/05/06/an-interview-with-michael-trier.html" >An interview with Michael Trier</a>
		  </li>
		  
		  <li>
		    <a href="/marketing/2008/05/06/popularizing-django-or-reusable-apps-considered-ha.html" >Popularizing Django -- Or Reusable apps considered harmful.</a>
		  </li>
		  
		  <li>
		    <a href="/interviews/2008/04/27/interview-with-james-bennett-django-release-manage.html" >Interview with James Bennett - Django release manager</a>
		  </li>
		  
		  <li>
		    <a href="/search/2008/04/22/parable-of-the-nofollow.html" >Parable of the nofollow</a>
		  </li>
		  
		  <li>
		    <a href="/startup/2008/04/22/why-people-start-startups.html" >Why people start startups.</a>
		  </li>
		  
		  <li>
		    <a href="/marketing/2008/04/21/marketing-lessons-from-google.html" >Marketing lessons from Google</a>
		  </li>
		  
		  <li>
		    <a href="/python/2008/04/18/five-things-i-hate-about-django.html" >Five Things I Hate About Django.</a>
		  </li>
		  
		  <li>
		    <a href="/startup/2008/04/12/first-step-to-startup-getting-your-pitch.html" >First step to startup - Getting your pitch</a>
		  </li>
		  
		  <li>
		    <a href="/python/2008/04/11/two-djangoappengine-tutorials.html" >Two Django+Appengine Tutorials</a>
		  </li>
		  
		  <li>
		    <a href="/python/2008/04/09/using-appengine-with-django-why-it-is-pretty-much.html" >Using Appengine with Django, why it is pretty much unusable</a>
		  </li>
		  
		  <li>
		    <a href="/python/2008/04/09/google-appengine-first-impressions.html" >Google Appengine - First Impressions</a>
		  </li>
		  
		</ul>
        </div>
        
        
        
      </section>
      <div id="content">
     	<p>This post tries to explain web application performance. <strong>Performance</strong> means the <strong>number of requests per second</strong> that can be served by a deployed application.</p>

<p>This post would help answer questions like:</p>

<ul>
  <li>How <code class="highlighter-rouge">performant</code> is an application.</li>
  <li>How much <code class="highlighter-rouge">load</code> can it handle.</li>
  <li>How many <code class="highlighter-rouge">concurrent requests</code> can it serve.</li>
  <li>How can you determine <code class="highlighter-rouge">requests per second</code> for an application.</li>
  <li>What steps to take to increase <code class="highlighter-rouge">serving capability</code> for an application.</li>
</ul>

<p>This post has as much code as theory.</p>

<p>This post assumes that you have a basic understanding of processes, threads. You can read our <a href="http://agiliq.com/blog/2013/09/process-and-threads-for-beginners/" target="_blank">previous post</a> for basic understanding of processes and threads.</p>

<p>In any web application there are many urls and associated handlers/controllers for each url. A url might respond in 200ms while another url might take 3 seconds. While determining performance of an application, choose the url which will be used most often and determine its performance.</p>

<h3 id="factors-which-determine-performance">Factors which determine performance</h3>

<p>There are many <code class="highlighter-rouge">factors</code> determining performance of an application. Major factors are:</p>

<ul>
  <li>Application type</li>
  <li>Application complexity</li>
  <li>Web server</li>
  <li>Physical server, i.e infrastructure</li>
  <li>Web server configuration</li>
</ul>

<p>In this post we will see how changing <code class="highlighter-rouge">web server configuration</code> changes performance with other factors remaining constant.</p>

<p>Two major components of <code class="highlighter-rouge">web server configuration</code> are:</p>

<ol>
  <li>Number of processes/workers</li>
  <li>Number of threads</li>
</ol>

<p>We will also see what web server configuration should be preferred for which application type. Example: How increasing number of server processes boosts performance for some applications while it reduces performance for other application types.</p>

<h4 id="application-type">Application type</h4>

<p>Applications could be compute intensive or network intensive or I/O intensive. Compute intensive applications can’t get any benefit by making number of server workers or threads greater than number of CPU cores.</p>

<p>Network intensive or I/O intensive applications can benefit a lot by making several server workers/threads run on each core.</p>

<h4 id="application-complexity">Application complexity</h4>

<p>Suppose a url handler makes two db calls and takes a second to respond. In such case reducing number of db calls to 1 will reduce response time by half and number of requests served per second will get doubled.</p>

<h4 id="web-server">Web server</h4>

<p>There are many web servers. eg: Apache, Gunicorn, uwsgi etc. Apache might be better than gunicorn and might be able to handle more requests per second than gunicorn.</p>

<h4 id="physical-serverinfrastructure">Physical server/Infrastructure</h4>

<p>Increasing the number of cores or memory will improve the performance. If a single core machine is able to handle 10 requests per second for a computationally intensive application, then a machine with 2 cores should be able to handle 20 requests per second.</p>

<p>This might need properly configuring the web server to get maximum utilization from physical server.</p>

<h4 id="application-server-configuration">Application server configuration</h4>

<p>Number of running web server processes influences performance. Similarly number of server threads in each process influences performance too.</p>

<h3 id="demo-server-configuration">Demo server configuration</h3>

<p>Our demo physical server is a t1.micro instance with 1 GB RAM. It has a single core.</p>

<p>Demo application for this post uses Django/Python served from a Gunicorn application server.</p>

<p>Familiarity with Django would be helpful but you should be able to follow as long as you have understanding of any web framework.</p>

<h3 id="demo-application">Demo application</h3>

<p>Our application has the following url:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>http://34.233.117.92:8000/static_content_sleep
</code></pre></div></div>

<p>The Django handler/controller for url <code class="highlighter-rouge">static_content_sleep</code> looks like:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def static_content_sleep(request):
	st = time.time()  # Compute Start time
	print "sleeping"
	time.sleep(1)     # Simulate db call which takes 1 seconds
	print "waking"
	print time.time() - st, "in this function"  # Compute end time
	return HttpResponse("Woke up")
</code></pre></div></div>

<p>Most web applications would be working with a database to fetch data.</p>

<p>We don’t have a database setup. We are assuming that db used by the handler responds in 1 second. We are simulating a db call by making the code execution sleep for 1 second.</p>

<p>This application is being served by Gunicorn web server. Gunicorn configuration looks like:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>workers = 1
threads = 1
bind = '0.0.0.0:8000'
daemon = False
</code></pre></div></div>

<p>The important configuration variables are <code class="highlighter-rouge">workers</code> and <code class="highlighter-rouge">threads</code>. Ignore others.</p>

<p>If you want to setup gunicorn on your physical server, you can refer our <a href="http://agiliq.com/blog/2013/08/minimal-nginx-and-gunicorn-configuration-for-djang/" target="_blank">previous post</a>.</p>

<p>We used default gunicorn configuration for <code class="highlighter-rouge">workers</code> and <code class="highlighter-rouge">threads</code>. The default configuration of gunicorn has following characteristics:</p>

<h4 id="workers">workers</h4>

<p>Default is 1. This means that only one gunicorn process would be running on the physical server.</p>

<p>If we make it 2, it would mean that 2 gunicorn processes would be running on the server.</p>

<h4 id="threads">threads</h4>

<p>Default is 1. This tells number of threads in each worker process. This means that each gunicorn worker is single threaded and isn’t multithreaded.</p>

<h3 id="making-requests">Making requests</h3>

<p>Let’s make two simultaneous request to this url.</p>

<p>Making two requests from browser will not be simultaneous as switching from one browser tab to another tab might take you more than a second. In that case you cannot properly observe time taken by the server to process two simultaneous requests.</p>

<p>Let’s write a Python function to make simultaneous requests and log the time.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>In [15]: from threading import Thread
In [16]: import requests

In [17]: class UrlThread(Thread):
	...:     def run(self):
	...:         resp = requests.get('http://34.233.117.92:8000/static_content_sleep')

In [18]: def make_n_requests(num_requests):
	...:     threads = []
	...:     for i in range(num_requests):
	...:         threads.append(UrlThread())
	...:     start = time.time()
			 # The requests will be almost simultaneous.
			 # Second request will be made within nanoseconds of making the first request.
	...:     for thread in threads:
	...:         thread.start() # Threads will be started without waiting for response of previous threads
	...:     for thread in threads:
	...:         thread.join()  # Wait for response for all the requests.
	...:     end = time.time()
	...:     print "Time to get response for %d simultaneous requests" % (num_requests,), end - start
</code></pre></div></div>

<p>You can use curl or a shell script or any programming language using which you can simulate <strong>n</strong> simultaneous requests.</p>

<p>Make 2 simultaneous requests</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>In [19]: make_n_requests(2)
Time to get response for 2 simultaneous requests 2.622205019
</code></pre></div></div>

<p>Gunicorn log on server looks like:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sleeping
waking
1.00131487846 in this function
sleeping
waking
1.00130581856 in this function
</code></pre></div></div>

<h4 id="observation">Observation</h4>

<ul>
  <li>Print statements of the handler are printed. Print statements are printed again.</li>
  <li>Based on prints, we can infer that second request’s execution started after completion of first request.</li>
</ul>

<p>Let’s make 5 simultaneous requests and see what happens</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>In [20]: make_n_requests(5)
Time to get response for 5 simultaneous requests 5.65813708305
</code></pre></div></div>

<p>Gunicorn log looks like:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sleeping
waking
1.00114989281 in this function
sleeping
waking
1.0011370182 in this function
sleeping
waking
1.00114202499 in this function
sleeping
waking
1.00130295753 in this function
sleeping
waking
1.00131988525 in this function
</code></pre></div></div>

<p>You can see 5 sets of print statements.</p>

<h4 id="observation-1">Observation</h4>

<ul>
  <li>As we increase number of simultaneous requests, response time is proportionately increasing.</li>
  <li>For 2 requests, server takes around 2 seconds to respond.</li>
  <li>For 5 requests, server takes around 5 seconds to respond.</li>
  <li>For 10 requests, it <code class="highlighter-rouge">would take</code> around 10 seconds to respond.</li>
</ul>

<p>Let’s change gunicorn <code class="highlighter-rouge">threads</code>from its default value of 1 to 5.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>threads = 5
</code></pre></div></div>

<p>Gunicorn process will have 5 threads now. And each thread is capable of serving a request. Hence 5 requests can be simultaneosly served.</p>

<p>Restart gunicorn.</p>

<p>Let’s make 5 simultaneous requests again.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>In [27]: make_n_requests(5)
Time to get response for 5 simultaneous requests 1.77244091034
</code></pre></div></div>

<p>** While earlier it was taking around 5 seconds to get 5 simultaneous requests processed, now it takes less than 2 seconds. **</p>

<p>Server log looks like:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sleeping
 sleeping
sleeping
sleeping
sleeping
waking
1.001278162 in this function
 waking
1.00262784958 in this function
waking
1.00353288651 in this function
waking
1.00325298309 in this function
waking
1.0035841465 in this function
</code></pre></div></div>

<h4 id="important-observation">Important Observation</h4>

<ul>
  <li>Each gunicorn thread could handle a request.</li>
  <li>Processor kept switching between all these 5 threads.</li>
  <li>This way all 5 requests being processed by 5 different threads got a chance to execute concurrently without waiting for completion of any request.</li>
  <li>Print pattern also suggests that execution of 5 requests started before waiting for completion of first request.</li>
</ul>

<p>We can accomplish this performance boost by increasing number of workers instead of number of threads. We will change threads back to 1 and increase <code class="highlighter-rouge">workers</code> to 5 instead.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>workers = 5
threads = 1
</code></pre></div></div>

<p>Make 5 simultaneous requests</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>In [22]: make_n_requests(5)
Time to get response for 5 simultaneous requests 1.76864600182
</code></pre></div></div>

<p>Server gunicorn log looks like:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sleeping
sleeping
sleeping
sleeping
sleeping
waking
1.00134015083 in this function
waking
1.00125384331 in this function
waking
1.00277590752 in this function
waking
1.00121593475 in this function
waking
1.00092220306 in this function
</code></pre></div></div>

<p>Here, processor executed 5 processes parallely.</p>

<p>Let’s make 10 simultaneous requests:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>In [23]: make_n_requests(10)
Time to get response for 10 simultaneous requests 2.74328804016
</code></pre></div></div>

<p>Server log looks like:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sleeping   -&gt; First 5 requests are assigned to 5 workers
sleeping
sleeping
sleeping
sleeping
waking     -&gt; One worker, say worker 1, finished execution.
waking     -&gt; Another worker, say worker 2, finished execution. This print was executed before last print statement of worker 1 could execute.
1.00396203995 in this function  -&gt; Worker 1 last print statement. Worker 1 free now and can serve another request.
1.00669813156 in this function  -&gt; Worker 2 last print statement. Worker 2 free now
sleeping   -&gt; Assigned to probably worker 1
waking     -&gt; Probably worker 3 woke up.
1.00510120392 in this function
sleeping
waking
1.00605106354 in this function
sleeping
sleeping
waking
1.00563693047 in this function
sleeping
waking
1.00345993042 in this function
waking
1.00495409966 in this function
waking
1.00393104553 in this function
waking
1.0027141571 in this function
waking
1.00151586533 in this function
</code></pre></div></div>

<h4 id="observation-2">Observation</h4>

<ul>
  <li>We made 10 simultaneous requests.</li>
  <li>First 5 requests were assigned to 5 running gunicorn processes.</li>
  <li>First 5 requests were concurrently handled during 1st second. 5 remaining requests were waiting to be executed during this time. During next second each gunicorn process picked up another request after completing a request. That’s why it took around 2 seconds to get response for 10 requests.</li>
</ul>

<p>Let’s use 2 threads with 5 workers</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>threads = 2
workers = 5
</code></pre></div></div>

<p>Let’s restart gunicorn. Now there are 5 gunicorn processes running and each process is running 2 threads.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>In [16]: make_n_requests(10)
Time to get response for 10 simultaneous requests 1.46275486946
</code></pre></div></div>

<p>Server log looks like:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sleeping -&gt; 'sleeping' repeated 10 times before any 'waking'.
sleeping
sleeping
sleeping
sleeping
sleeping
sleeping
sleeping
sleeping
sleeping
waking
1.00119280815 in this function
waking
1.00499987602 in this function
waking
1.00233006477 in this function
waking
1.00144195557 in this function
waking
1.00114202499 in this function
waking
1.00117397308 in this function
waking
1.00114512444 in this function
waking
1.00120997429 in this function
waking
1.00124192238 in this function
waking
1.00117397308 in this function
</code></pre></div></div>

<h3 id="inference">Inference</h3>

<p>Based on these examples we can infer that making number of threads/workers greater than number of cores improves the performance for a network intensive and I/O intensive application.</p>

<h4 id="can-we-keep-increasing-workers-and-threads">Can we keep increasing workers and threads</h4>

<p>You have to keep RAM usage under consideration while tuning the number of workers and threads.</p>

<p>Code execution for a handler needs memory. While worker is processing a request, sufficient memory must be available. If handling each request needs 50 MB and you have 5 workers and 1 thread running, you must ensure that 250 MB free RAM is there after starting gunicorn.</p>

<h4 id="performance-with-current-configuration">Performance with current configuration</h4>

<p>Curent configuration has 2 workers and 5 threads for each worker. So 10 requests will be handled concurrently.</p>

<p>Each request takes around 1 second to respond.</p>

<p>So with current configuration server can handle 10 requests per second.</p>

<p>If the handler is changed to <code class="highlighter-rouge">time.sleep(0.5)</code>, i.e if each request could respond in approximately 0.5 seconds then server would be able to handle 20 requests per second.</p>

<p>If we change number of workers to 3 and 5 threads for each worker, then with time.sleep(0.5), server would be able to handle 3*5*2, i,e 30 requests per second.</p>

<h4 id="increase-performance-by-increasing-number-of-cores">Increase performance by increasing number of cores</h4>

<p>Suppose we use a machine with 2 cores.</p>

<p>On single core machine if we were using 2 workers, then on double core machine we should use 4 workers. This assumes that there is sufficient RAM available to be allocated to 4 workers.</p>

<p>A dual core machine would be able to handle 4*5, i.e 20 requests per second, assuming each request responds in a second.</p>

<p>If we found that 3 workers with 5 thread each is an optimum combination on a single core machine, then we should use 6 workers on a dual core machine.</p>

<h3 id="computationally-bound-applications">Computationally bound applications</h3>

<p>Earlier sections discussed about I/O bound applications. Let’s talk about CPU bound applications.</p>

<p>App has a following url:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>http://34.233.117.92:8000/list_of_dict
</code></pre></div></div>

<p>The Django handler/controller for url <code class="highlighter-rouge">list_of_dict</code> looks like:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def list_of_dict(request):
	print "entered function"
    st = time.time()
    for i in xrange(30000000):
            pass
    print time.time() - st, "in this function"
    return HttpResponse(json.dumps("a"))
</code></pre></div></div>

<p>Let’s change <code class="highlighter-rouge">UrlThread</code> used by <code class="highlighter-rouge">make_n_requests</code> to work with this new url.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>In [14]: class UrlThread(Thread):
	...:     def run(self):
	...:         resp = requests.get('http://34.233.117.92:8000/list_of_dict')
</code></pre></div></div>

<p>Let’s change gunicorn configuration to have a single worker and single thread.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>workers = 1
threads = 1
</code></pre></div></div>

<p>Let’s make 1 request to this url</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>In [28]: make_n_requests(1)
Time to get response for 1 simultaneous requests 1.49780297279
</code></pre></div></div>

<p>Server log looks like:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>entered function
0.454895019531 in this function
</code></pre></div></div>

<p>It takes around 0.5 seconds in the handler as you can see from server log. And it takes around 1 second for request and response to travel on the wire. So in total it takes around 1.5 seconds.</p>

<p>Let’s make 5 requests to this url</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>In [32]: make_n_requests(5)
Time to get response for 5 simultaneous requests 2.94885587692
</code></pre></div></div>

<p>Server log looks like:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>entered function
0.460360050201 in this function
entered function
0.459032058716 in this function
entered function
0.462526082993 in this function
entered function
0.45965385437 in this function
entered function
0.460576057434 in this function
</code></pre></div></div>

<p>Each request takes around 0.5 seconds to complete. Plus there is an additional time for request and response to move over the wire. In total it takes around 3 seconds.</p>

<p>Time to get response for n requests is increasing linearly as we increase n.</p>

<p>With a single worker and single thread, time to get response for simultaneous requests was increasing linearly with number of requests in I/O bound handler too.</p>

<p>Let’s change gunicorn <code class="highlighter-rouge">threads</code>from its default value of 1 to 5.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>threads = 5
</code></pre></div></div>

<p>Let’s make 5 simultaneous requests again.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>In [33]: make_n_requests(5)
Time to get response for 5 simultaneous requests 3.37198781967
</code></pre></div></div>

<p>We didn’t get any advantage by increasing number of threads. Instead the performance deteriorated.</p>

<p>Server log looks like:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>entered function
entered function
entered function
entered function
entered function
2.68351387978 in this function
 2.67835998535 in this function
2.67423701286 in this function
 2.67234015465 in this function
2.6794462204 in this function
</code></pre></div></div>

<p>Event different requests didn’t complete in 0.5 second as was happening with single thread. The CPU time was split between 5 threads and so instead of 0.5 seconds it took 2.6 seconds for each request to complete.</p>

<h4 id="observation-3">Observation</h4>

<ul>
  <li>In CPU bound applications, CPU isn’t idle. So multiple threads don’t provide an advantage.</li>
  <li>CPU time is split between threads, which infact leads to longer response time for each request.</li>
  <li>Thread switching brings a performance hit. That’s why we saw response time for 5 requests going up from 2.9 seconds to 3.3 seconds for 5 simultaneous requests.</li>
</ul>

<h3 id="takeaways">Takeaways</h3>

<ul>
  <li>There is no definite answer for how many workers and threads will provide maximum performance. It’s a matter of tuning and finding out.</li>
  <li>Making number of workers/threads greater than number of cores almost always leads to higher number of requests per second handling in a I/O bound application.</li>
  <li>Making number of workers/threads greater than number of cores almost always leads to reduced number of requests per second handling in a CPU bound application.</li>
  <li>Making number of workers/threads greater than number of cores leads to increase in response time for each request in a CPU bound application.</li>
</ul>


       </div>
    </div>

    
  </body>
</html>

